The Pragmatic Programmer


What Makes a Pragmatic Programmer?


Each developer is unique, with individual strengths and weaknesses, preferences and dislikes. Over time, each will craft their own personal environment. That environment will reflect the programmer’s individuality just as forcefully as his or her hobbies, clothing, or haircut. However, if you’re a Pragmatic Programmer, you’ll share many of the following characteristics:

Early adopter/fast adapter

You have an instinct for technologies and techniques, and you love trying things out. When given something new, you can grasp it quickly and integrate it with the rest of your knowledge. Your confidence is born of experience.


Inquisitive

You tend to ask questions. That’s neat—how did you do that? Did you have problems with that library? What’s this quantum computing I’ve heard about? How are symbolic links implemented? You are a pack rat for little facts, each of which may affect some decision years from now.


Critical thinker

You rarely take things as given without first getting the facts. When colleagues say “because that’s the way it’s done,” or a vendor promises the solution to all your problems, you smell a challenge.


Realistic

You try to understand the underlying nature of each problem you face. This realism gives you a good feel for how difficult things are, and how long things will take. Deeply understanding that a process should be difficult or will take a while to complete gives you the stamina to keep at it.


Jack of all trades

You try hard to be familiar with a broad range of technologies and environments, and you work to keep abreast of new developments. Although your current job may require you to be a specialist, you will always be able to move on to new areas and new challenges.


We’ve left the most basic characteristics until last. All Pragmatic Programmers share them. They’re basic enough to state as tips:

Tip 1 Care About Your Craft

We feel that there is no point in developing software unless you care about doing it well.

Tip 2 Think! About Your Work

In order to be a Pragmatic Programmer, we’re challenging you to think about what you’re doing while you’re doing it. This isn’t a one-time audit of current practices—it’s an ongoing critical appraisal of every decision you make, every day, and on every project. Never run on auto-pilot. Constantly be thinking, critiquing your work in real time. The old IBM corporate motto, THINK!, is the Pragmatic Programmer’s mantra.

If this sounds like hard work to you, then you’re exhibiting the realistic characteristic. This is going to take up some of your valuable time—time that is probably already under tremendous pressure. The reward is a more active involvement with a job you love, a feeling of mastery over an increasing range of subjects, and pleasure in a feeling of continuous improvement. Over the long term, your time investment will be repaid as you and your team become more efficient, write code that’s easier to maintain, and spend less time in meetings.


Individual Pragmatists, Large Teams


Some people feel that there is no room for individuality on large teams or complex projects. “Software is an engineering discipline,” they say, “that breaks down if individual team members make decisions for themselves.”

We strongly disagree.

There should be engineering in software construction. However, this doesn’t preclude individual craftsmanship. Think about the large cathedrals built in Europe during the Middle Ages. Each took thousands of person-years of effort, spread over many decades. Lessons learned were passed down to the next set of builders, who advanced the state of structural engineering with their accomplishments. But the carpenters, stonecutters, carvers, and glass workers were all craftspeople, interpreting the engineering requirements to produce a whole that transcended the purely mechanical side of the construction. It was their belief in their individual contributions that sustained the projects: We who cut mere stones must always be envisioning cathedrals.

Within the overall structure of a project there is always room for individuality and craftsmanship. This is particularly true given the current state of software engineering. One hundred years from now, our engineering may seem as archaic as the techniques used by medieval cathedral builders seem to today’s civil engineers, while our craftsmanship will still be honored.


It’s a Continuous Process


A tourist visiting England’s Eton College asked the gardener how he got the lawns so perfect. “That’s easy,” he replied, “You just brush off the dew every morning, mow them every other day, and roll them once a week.”

“Is that all?” asked the tourist. “Absolutely,” replied the gardener. “Do that for 500 years and you’ll have a nice lawn, too.”

Great lawns need small amounts of daily care, and so do great programmers. Management consultants like to drop the word kaizen in conversations. “Kaizen” is a Japanese term that captures the concept of continuously making many small improvements. It was considered to be one of the main reasons for the dramatic gains in productivity and quality in Japanese manufacturing and was widely copied throughout the world. Kaizen applies to individuals, too. Every day, work to refine the skills you have and to add new tools to your repertoire. Unlike the Eton lawns, you’ll start seeing results in a matter of days. Over the years, you’ll be amazed at how your experience has blossomed and how your skills have grown.





Chapter 1


A Pragmatic Philosophy


This book is about you.

Make no mistake, it is your career, and more importantly, Topic 1, ​It’s Your Life​. You own it. You’re here because you know you can become a better developer and help others become better as well. You can become a Pragmatic Programmer.

What distinguishes Pragmatic Programmers? We feel it’s an attitude, a style, a philosophy of approaching problems and their solutions. They think beyond the immediate problem, placing it in its larger context and seeking out the bigger picture. After all, without this larger context, how can you be pragmatic? How can you make intelligent compromises and informed decisions?

Another key to their success is that Pragmatic Programmers take responsibility for everything they do, which we discuss in Topic 2, ​The Cat Ate My Source Code​. Being responsible, Pragmatic Programmers won’t sit idly by and watch their projects fall apart through neglect. In Topic 3, ​Software Entropy​, we tell you how to keep your projects pristine.

Most people find change difficult, sometimes for good reasons, sometimes because of plain old inertia. In Topic 4, ​Stone Soup and Boiled Frogs​, we look at a strategy for instigating change and (in the interests of balance) present the cautionary tale of an amphibian that ignored the dangers of gradual change.

One of the benefits of understanding the context in which you work is that it becomes easier to know just how good your software has to be. Sometimes near-perfection is the only option, but often there are trade-offs involved. We explore this in Topic 5, ​Good-Enough Software​.

Of course, you need to have a broad base of knowledge and experience to pull all of this off. Learning is a continuous and ongoing process. In Topic 6, ​Your Knowledge Portfolio​, we discuss some strategies for keeping the momentum up.

Finally, none of us works in a vacuum. We all spend a large amount of time interacting with others. Topic 7, ​Communicate!​ lists ways we can do this better.

Pragmatic programming stems from a philosophy of pragmatic thinking. This chapter sets the basis for that philosophy.


Topic 1

It’s Your Life


I’m not in this world to live up to your expectations and you’re not in this world to live up to mine.


Bruce Lee


It is your life. You own it. You run it. You create it.

Many developers we talk to are frustrated. Their concerns are varied. Some feel they’re stagnating in their job, others that technology has passed them by. Folks feel they are under appreciated, or underpaid, or that their teams are toxic. Maybe they want to move to Asia, or Europe, or work from home.

And the answer we give is always the same.

“Why can’t you change it?”

Software development must appear close to the top of any list of careers where you have control. Our skills are in demand, our knowledge crosses geographic boundaries, we can work remotely. We’re paid well. We really can do just about anything we want.

But, for some reason, developers seem to resist change. They hunker down, and hope things will get better. They look on, passively, as their skills become dated and complain that their companies don’t train them. They look at ads for exotic locations on the bus, then step off into the chilling rain and trudge into work.

So here’s the most important tip in the book.

Tip 3 You Have Agency

Does your work environment suck? Is your job boring? Try to fix it. But don’t try forever. As Martin Fowler says, “you can change your organization or change your organization.”[3]

If technology seems to be passing you by, make time (in your own time) to study new stuff that looks interesting. You’re investing in yourself, so doing it while you’re off-the-clock is only reasonable.

Want to work remotely? Have you asked? If they say no, then find someone who says yes.

This industry gives you a remarkable set of opportunities. Be proactive, and take them.


Related Sections Include


Topic 4, ​Stone Soup and Boiled Frogs​

Topic 6, ​Your Knowledge Portfolio​


Topic 2

The Cat Ate My Source Code


	The greatest of all weaknesses is the fear of appearing weak.


J.B. Bossuet, Politics from Holy Writ, 1709


One of the cornerstones of the pragmatic philosophy is the idea of taking responsibility for yourself and your actions in terms of your career advancement, your learning and education, your project, and your day-to-day work. Pragmatic Programmers take charge of their own career, and aren’t afraid to admit ignorance or error. It’s not the most pleasant aspect of programming, to be sure, but it will happen—even on the best of projects. Despite thorough testing, good documentation, and solid automation, things go wrong. Deliveries are late. Unforeseen technical problems come up.

These things happen, and we try to deal with them as professionally as we can. This means being honest and direct. We can be proud of our abilities, but we must own up to our shortcomings—our ignorance and our mistakes.


Team Trust


Above all, your team needs to be able to trust and rely on you—and you need to be comfortable relying on each of them as well. Trust in a team is absolutely essential for creativity and collaboration according to the research literature.[4] In a healthy environment based in trust, you can safely speak your mind, present your ideas, and rely on your team members who can in turn rely on you. Without trust, well…

Imagine a high-tech, stealth ninja team infiltrating the villain’s evil lair. After months of planning and delicate execution, you’ve made it on site. Now it’s your turn to set up the laser guidance grid: “Sorry, folks, I don’t have the laser. The cat was playing with the red dot and I left it at home.”

That sort of breach of trust might be hard to repair.


Take Responsibility


Responsibility is something you actively agree to. You make a commitment to ensure that something is done right, but you don’t necessarily have direct control over every aspect of it. In addition to doing your own personal best, you must analyze the situation for risks that are beyond your control. You have the right not to take on a responsibility for an impossible situation, or one in which the risks are too great, or the ethical implications too sketchy. You’ll have to make the call based on your own values and judgment.

When you do accept the responsibility for an outcome, you should expect to be held accountable for it. When you make a mistake (as we all do) or an error in judgment, admit it honestly and try to offer options.

Don’t blame someone or something else, or make up an excuse. Don’t blame all the problems on a vendor, a programming language, management, or your coworkers. Any and all of these may play a role, but it is up to you to provide solutions, not excuses.

If there was a risk that the vendor wouldn’t come through for you, then you should have had a contingency plan. If your mass storage melts—taking all of your source code with it—and you don’t have a backup, it’s your fault. Telling your boss “the cat ate my source code’’ just won’t cut it.

Tip 4 Provide Options, Don’t Make Lame Excuses

Before you approach anyone to tell them why something can’t be done, is late, or is broken, stop and listen to yourself. Talk to the rubber duck on your monitor, or the cat. Does your excuse sound reasonable, or stupid? How’s it going to sound to your boss?

Run through the conversation in your mind. What is the other person likely to say? Will they ask, “Have you tried this…” or “Didn’t you consider that?” How will you respond? Before you go and tell them the bad news, is there anything else you can try? Sometimes, you just know what they are going to say, so save them the trouble.

Instead of excuses, provide options. Don’t say it can’t be done; explain what can be done to salvage the situation. Does code have to be deleted? Tell them so, and explain the value of refactoring (see Topic 40, ​Refactoring​).

Do you need to spend time prototyping to determine the best way to proceed (see Topic 13, ​Prototypes and Post-it Notes​)? Do you need to introduce better testing (see Topic 41, ​Test to Code​, and ​Ruthless and Continuous Testing​) or automation to prevent it from happening again?

Perhaps you need additional resources to complete this task. Or maybe you need to spend more time with the users? Or maybe it’s just you: do you need to learn some technique or technology in greater depth? Would a book or a course help? Don’t be afraid to ask, or to admit that you need help.

Try to flush out the lame excuses before voicing them aloud. If you must, tell your cat first. After all, if little Tiddles is going to take the blame….


Related Sections Include


Topic 49, ​Pragmatic Teams​


Challenges


How do you react when someone—such as a bank teller, an auto mechanic, or a clerk—comes to you with a lame excuse? What do you think of them and their company as a result?


When you find yourself saying, “I don’t know,” be sure to follow it up with “—but I’ll find out.” It’s a great way to admit what you don’t know, but then take responsibility like a pro.


Topic 3

Software Entropy


While software development is immune from almost all physical laws, the inexorable increase in entropy hits us hard. Entropy is a term from physics that refers to the amount of “disorder” in a system. Unfortunately, the laws of thermodynamics guarantee that the entropy in the universe tends toward a maximum. When disorder increases in software, we call it “software rot.” Some folks might call it by the more optimistic term, “technical debt,” with the implied notion that they’ll pay it back someday. They probably won’t.

Whatever the name, though, both debt and rot can spread uncontrollably.

There are many factors that can contribute to software rot. The most important one seems to be the psychology, or culture, at work on a project. Even if you are a team of one, your project’s psychology can be a very delicate thing. Despite the best-laid plans and the best people, a project can still experience ruin and decay during its lifetime. Yet there are other projects that, despite enormous difficulties and constant setbacks, successfully fight nature’s tendency toward disorder and manage to come out pretty well.

What makes the difference?

In inner cities, some buildings are beautiful and clean, while others are rotting hulks. Why? Researchers in the field of crime and urban decay discovered a fascinating trigger mechanism, one that very quickly turns a clean, intact, inhabited building into a smashed and abandoned derelict.[5]

A broken window.

One broken window, left unrepaired for any substantial length of time, instills in the inhabitants of the building a sense of abandonment—a sense that the powers that be don’t care about the building. So another window gets broken. People start littering. Graffiti appears. Serious structural damage begins. In a relatively short span of time, the building becomes damaged beyond the owner’s desire to fix it, and the sense of abandonment becomes reality.

Why would that make a difference? Psychologists have done studies[6] that show hopelessness can be contagious. Think of the flu virus in close quarters. Ignoring a clearly broken situation reinforces the ideas that perhaps nothing can be fixed, that no one cares, all is doomed; all negative thoughts which can spread among team members, creating a vicious spiral.

Tip 5 Don’t Live with Broken Windows

Don’t leave “broken windows’’ (bad designs, wrong decisions, or poor code) unrepaired. Fix each one as soon as it is discovered. If there is insufficient time to fix it properly, then board it up. Perhaps you can comment out the offending code, or display a “Not Implemented” message, or substitute dummy data instead. Take some action to prevent further damage and to show that you’re on top of the situation.

We’ve seen clean, functional systems deteriorate pretty quickly once windows start breaking. There are other factors that can contribute to software rot, and we’ll touch on some of them elsewhere, but neglect accelerates the rot faster than any other factor.

You may be thinking that no one has the time to go around cleaning up all the broken glass of a project. If so, then you’d better plan on getting a dumpster, or moving to another neighborhood. Don’t let entropy win.


First, Do No Harm


Andy once had an acquaintance who was obscenely rich. His house was immaculate, loaded with priceless antiques, objets d’art, and so on. One day, a tapestry that was hanging a little too close to a fireplace caught on fire. The fire department rushed in to save the day—and his house. But before they dragged their big, dirty hoses into the house, they stopped—with the fire raging—to roll out a mat between the front door and the source of the fire.

They didn’t want to mess up the carpet.

Now that sounds pretty extreme. Surely the fire department’s first priority is to put out the fire, collateral damage be damned. But they clearly had assessed the situation, were confident of their ability to manage the fire, and were careful not to inflict unnecessary damage to the property. That’s the way it must be with software: don’t cause collateral damage just because there’s a crisis of some sort. One broken window is one too many.

One broken window—a badly designed piece of code, a poor management decision that the team must live with for the duration of the project—is all it takes to start the decline. If you find yourself working on a project with quite a few broken windows, it’s all too easy to slip into the mindset of “All the rest of this code is crap, I’ll just follow suit.” It doesn’t matter if the project has been fine up to this point. In the original experiment leading to the “Broken Window Theory,” an abandoned car sat for a week untouched. But once a single window was broken, the car was stripped and turned upside down within hours.

By the same token, if you find yourself on a project where the code is pristinely beautiful—cleanly written, well designed, and elegant—you will likely take extra special care not to mess it up, just like the firefighters. Even if there’s a fire raging (deadline, release date, trade show demo, etc.), you don’t want to be the first one to make a mess and inflict additional damage.

Just tell yourself, “No broken windows.”


Related Sections Include


Topic 10, ​Orthogonality​

Topic 40, ​Refactoring​

Topic 44, ​Naming Things​


Challenges


Help strengthen your team by surveying your project neighborhood. Choose two or three broken windows and discuss with your colleagues what the problems are and what could be done to fix them.


Can you tell when a window first gets broken? What is your reaction? If it was the result of someone else’s decision, or a management edict, what can you do about it?


Topic 4

Stone Soup and Boiled Frogs


The three soldiers returning home from war were hungry. When they saw the village ahead their spirits lifted—they were sure the villagers would give them a meal. But when they got there, they found the doors locked and the windows closed. After many years of war, the villagers were short of food, and hoarded what they had.

Undeterred, the soldiers boiled a pot of water and carefully placed three stones into it. The amazed villagers came out to watch.

“This is stone soup,” the soldiers explained. “Is that all you put in it?” asked the villagers. “Absolutely—although some say it tastes even better with a few carrots…” A villager ran off, returning in no time with a basket of carrots from his hoard.

A couple of minutes later, the villagers again asked “Is that it?”

“Well,” said the soldiers, “a couple of potatoes give it body.” Off ran another villager.

Over the next hour, the soldiers listed more ingredients that would enhance the soup: beef, leeks, salt, and herbs. Each time a different villager would run off to raid their personal stores.

Eventually they had produced a large pot of steaming soup. The soldiers removed the stones, and they sat down with the entire village to enjoy the first square meal any of them had eaten in months.

There are a couple of morals in the stone soup story. The villagers are tricked by the soldiers, who use the villagers’ curiosity to get food from them. But more importantly, the soldiers act as a catalyst, bringing the village together so they can jointly produce something that they couldn’t have done by themselves—a synergistic result. Eventually everyone wins.

Every now and then, you might want to emulate the soldiers.

You may be in a situation where you know exactly what needs doing and how to do it. The entire system just appears before your eyes—you know it’s right. But ask permission to tackle the whole thing and you’ll be met with delays and blank stares. People will form committees, budgets will need approval, and things will get complicated. Everyone will guard their own resources. Sometimes this is called “start-up fatigue.’’

It’s time to bring out the stones. Work out what you can reasonably ask for. Develop it well. Once you’ve got it, show people, and let them marvel. Then say “of course, it would be better if we added…’’ Pretend it’s not important. Sit back and wait for them to start asking you to add the functionality you originally wanted. People find it easier to join an ongoing success. Show them a glimpse of the future and you’ll get them to rally around.[7]

Tip 6 Be a Catalyst for Change


The Villagers’ Side


On the other hand, the stone soup story is also about gentle and gradual deception. It’s about focusing too tightly. The villagers think about the stones and forget about the rest of the world. We all fall for it, every day. Things just creep up on us.

We’ve all seen the symptoms. Projects slowly and inexorably get totally out of hand. Most software disasters start out too small to notice, and most project overruns happen a day at a time. Systems drift from their specifications feature by feature, while patch after patch gets added to a piece of code until there’s nothing of the original left. It’s often the accumulation of small things that breaks morale and teams.

Tip 7 Remember the Big Picture

We’ve never tried this—honest. But “they” say that if you take a frog and drop it into boiling water, it will jump straight back out again. However, if you place the frog in a pan of cold water, then gradually heat it, the frog won’t notice the slow increase in temperature and will stay put until cooked.

Note that the frog’s problem is different from the broken windows issue discussed in Topic 3, ​Software Entropy​. In the Broken Window Theory, people lose the will to fight entropy because they perceive that no one else cares. The frog just doesn’t notice the change.

Don’t be like the fabled frog. Keep an eye on the big picture. Constantly review what’s happening around you, not just what you personally are doing.


Related Sections Include


Topic 1, ​It’s Your Life​

Topic 38, ​Programming by Coincidence​


Challenges


While reviewing a draft of the first edition, John Lakos raised the following issue: The soldiers progressively deceive the villagers, but the change they catalyze does them all good. However, by progressively deceiving the frog, you’re doing it harm. Can you determine whether you’re making stone soup or frog soup when you try to catalyze change? Is the decision subjective or objective?


Quick, without looking, how many lights are in the ceiling above you? How many exits in the room? How many people? Is there anything out of context, anything that looks like it doesn’t belong? This is an exercise in situational awareness, a technique practiced by folks ranging from Boy and Girl Scouts to Navy SEALs. Get in the habit of really looking and noticing your surroundings. Then do the same for your project.


Topic 5

Good-Enough Software


	Striving to better, oft we mar what’s well.


Shakespeare, King Lear 1.4


There’s an old(ish) joke about a company that places an order for 100,000 ICs with a Japanese manufacturer. Part of the specification was the defect rate: one chip in 10,000. A few weeks later the order arrived: one large box containing thousands of ICs, and a small one containing just ten. Attached to the small box was a label that read: “These are the faulty ones.’’

If only we really had this kind of control over quality. But the real world just won’t let us produce much that’s truly perfect, particularly not bug-free software. Time, technology, and temperament all conspire against us.

However, this doesn’t have to be frustrating. As Ed Yourdon described in an article in IEEE Software, When good-enough software is best [You95], you can discipline yourself to write software that’s good enough—good enough for your users, for future maintainers, for your own peace of mind. You’ll find that you are more productive and your users are happier. And you may well find that your programs are actually better for their shorter incubation.

Before we go any further, we need to qualify what we’re about to say. The phrase “good enough’’ does not imply sloppy or poorly produced code. All systems must meet their users’ requirements to be successful, and meet basic performance, privacy, and security standards. We are simply advocating that users be given an opportunity to participate in the process of deciding when what you’ve produced is good enough for their needs.


Involve Your Users in the Trade-Off


Normally you’re writing software for other people. Often you’ll remember to find out what they want.[8] But do you ever ask them how good they want their software to be? Sometimes there’ll be no choice. If you’re working on pacemakers, an autopilot, or a low-level library that will be widely disseminated, the requirements will be more stringent and your options more limited.

However, if you’re working on a brand-new product, you’ll have different constraints. The marketing people will have promises to keep, the eventual end users may have made plans based on a delivery schedule, and your company will certainly have cash-flow constraints. It would be unprofessional to ignore these users’ requirements simply to add new features to the program, or to polish up the code just one more time. We’re not advocating panic: it is equally unprofessional to promise impossible time scales and to cut basic engineering corners to meet a deadline.

The scope and quality of the system you produce should be discussed as part of that system’s requirements.

Tip 8 Make Quality a Requirements Issue

Often you’ll be in situations where trade-offs are involved. Surprisingly, many users would rather use software with some rough edges today than wait a year for the shiny, bells-and-whistles version (and in fact what they will need a year from now may be completely different anyway). Many IT departments with tight budgets would agree. Great software today is often preferable to the fantasy of perfect software tomorrow. If you give your users something to play with early, their feedback will often lead you to a better eventual solution (see Topic 12, ​Tracer Bullets​).


Know When to Stop


In some ways, programming is like painting. You start with a blank canvas and certain basic raw materials. You use a combination of science, art, and craft to determine what to do with them. You sketch out an overall shape, paint the underlying environment, then fill in the details. You constantly step back with a critical eye to view what you’ve done. Every now and then you’ll throw a canvas away and start again.

But artists will tell you that all the hard work is ruined if you don’t know when to stop. If you add layer upon layer, detail over detail, the painting becomes lost in the paint.

Don’t spoil a perfectly good program by overembellishment and overrefinement. Move on, and let your code stand in its own right for a while. It may not be perfect. Don’t worry: it could never be perfect. (In Chapter 7, ​While You Are Coding​, we’ll discuss philosophies for developing code in an imperfect world.)


Related Sections Include


Topic 45, ​The Requirements Pit​

Topic 46, ​Solving Impossible Puzzles​


Challenges


Look at the software tools and operating systems that you use regularly. Can you find any evidence that these organizations and/or developers are comfortable shipping software they know is not perfect? As a user, would you rather (1) wait for them to get all the bugs out, (2) have complex software and accept some bugs, or (3) opt for simpler software with fewer defects?


Consider the effect of modularization on the delivery of software. Will it take more or less time to get a tightly coupled monolithic block of software to the required quality compared with a system designed as very loosely coupled modules or microservices? What are the advantages or disadvantages of each approach?


Can you think of popular software that suffers from feature bloat? That is, software containing far more features than you would ever use, each feature introducing more opportunity for bugs and security vulnerabilities, and making the features you do use harder to find and manage. Are you in danger of falling into this trap yourself?


Topic 6

Your Knowledge Portfolio


An investment in knowledge always pays the best interest.


Benjamin Franklin


Ah, good old Ben Franklin—never at a loss for a pithy homily. Why, if we could just be early to bed and early to rise, we’d be great programmers—right? The early bird might get the worm, but what happens to the early worm?

In this case, though, Ben really hit the nail on the head. Your knowledge and experience are your most important day-to-day professional assets.

Unfortunately, they’re expiring assets.[9] Your knowledge becomes out of date as new techniques, languages, and environments are developed. Changing market forces may render your experience obsolete or irrelevant. Given the ever-increasing pace of change in our technological society, this can happen pretty quickly.

As the value of your knowledge declines, so does your value to your company or client. We want to prevent this from ever happening.

Your ability to learn new things is your most important strategic asset. But how do you learn how to learn, and how do you know what to learn?


Your Knowledge Portfolio


We like to think of all the facts programmers know about computing, the application domains they work in, and all their experience as their knowledge portfolios. Managing a knowledge portfolio is very similar to managing a financial portfolio:

Serious investors invest regularly—as a habit.

Diversification is the key to long-term success.

Smart investors balance their portfolios between conservative and high-risk, high-reward investments.

Investors try to buy low and sell high for maximum return.

Portfolios should be reviewed and rebalanced periodically.


To be successful in your career, you must invest in your knowledge portfolio using these same guidelines.

The good news is that managing this kind of investment is a skill just like any other—it can be learned. The trick is to make yourself do it initially and form a habit. Develop a routine which you follow until your brain internalizes it. At that point, you’ll find yourself sucking up new knowledge automatically.


Building Your Portfolio


Invest regularly

Just as in financial investing, you must invest in your knowledge portfolio regularly, even if it’s just a small amount. The habit is as important as the sums, so plan to use a consistent time and place, away from interruptions. A few sample goals are listed in the next section.


Diversify

The more different things you know, the more valuable you are. As a baseline, you need to know the ins and outs of the particular technology you are working with currently. But don’t stop there. The face of computing changes rapidly—hot technology today may well be close to useless (or at least not in demand) tomorrow. The more technologies you are comfortable with, the better you will be able to adjust to change. And don’t forget all the other skills you need, including those in non-technical areas.


Manage risk

Technology exists along a spectrum from risky, potentially high-reward to low-risk, low-reward standards. It’s not a good idea to invest all of your money in high-risk stocks that might collapse suddenly, nor should you invest all of it conservatively and miss out on possible opportunities. Don’t put all your technical eggs in one basket.


Buy low, sell high

Learning an emerging technology before it becomes popular can be just as hard as finding an undervalued stock, but the payoff can be just as rewarding. Learning Java back when it was first introduced and unknown may have been risky at the time, but it paid off handsomely for the early adopters when it became an industry mainstay later.


Review and rebalance

This is a very dynamic industry. That hot technology you started investigating last month might be stone cold by now. Maybe you need to brush up on that database technology that you haven’t used in a while. Or perhaps you could be better positioned for that new job opening if you tried out that other language….


Of all these guidelines, the most important one is the simplest to do:

Tip 9 Invest Regularly in Your Knowledge Portfolio


Goals


Now that you have some guidelines on what and when to add to your knowledge portfolio, what’s the best way to go about acquiring intellectual capital with which to fund your portfolio? Here are a few suggestions:

Learn at least one new language every year

Different languages solve the same problems in different ways. By learning several different approaches, you can help broaden your thinking and avoid getting stuck in a rut. Additionally, learning many languages is easy thanks to the wealth of freely available software.


Read a technical book each month

While there’s a glut of short-form essays and occasionally reliable answers on the web, for deep understanding you need long-form books. Browse the booksellers for technical books on interesting topics related to your current project.[10] Once you’re in the habit, read a book a month. After you’ve mastered the technologies you’re currently using, branch out and study some that don’t relate to your project.


Read nontechnical books, too

It is important to remember that computers are used by people—people whose needs you are trying to satisfy. You work with people, are employed by people, and get hacked by people. Don’t forget the human side of the equation, as that requires an entirely different skill set (we ironically call these soft skills, but they are actually quite hard to master).


Take classes

Look for interesting courses at a local or online college or university, or perhaps at the next nearby trade show or conference.


Participate in local user groups and meetups

Isolation can be deadly to your career; find out what people are working on outside of your company. Don’t just go and listen: actively participate.


Experiment with different environments

If you’ve worked only in Windows, spend some time with Linux. If you’ve used only makefiles and an editor, try a sophisticated IDE with cutting-edge features, and vice versa.


Stay current

Read news and posts online on technology different from that of your current project. It’s a great way to find out what experiences other people are having with it, the particular jargon they use, and so on.


It’s important to continue investing. Once you feel comfortable with some new language or bit of technology, move on. Learn another one.

It doesn’t matter whether you ever use any of these technologies on a project, or even whether you put them on your resume. The process of learning will expand your thinking, opening you to new possibilities and new ways of doing things. The cross-pollination of ideas is important; try to apply the lessons you’ve learned to your current project. Even if your project doesn’t use that technology, perhaps you can borrow some ideas. Get familiar with object orientation, for instance, and you’ll write procedural programs differently. Understand the functional programming paradigm and you’ll write object-oriented code differently, and so on.


Opportunities for Learning


So you’re reading voraciously, you’re on top of all the latest breaking developments in your field (not an easy thing to do), and somebody asks you a question. You don’t have the faintest idea what the answer is, and freely admit as much.

Don’t let it stop there. Take it as a personal challenge to find the answer. Ask around. Search the web—the scholarly parts too, not just the consumer parts.

If you can’t find the answer yourself, find out who can. Don’t let it rest. Talking to other people will help build your personal network, and you may surprise yourself by finding solutions to other, unrelated problems along the way. And that old portfolio just keeps getting bigger….

All of this reading and researching takes time, and time is already in short supply. So you need to plan ahead. Always have something to read in an otherwise dead moment. Time spent waiting for doctors and dentists can be a great opportunity to catch up on your reading—but be sure to bring your own e-reader with you, or you might find yourself thumbing through a dog-eared 1973 article about Papua New Guinea.


Critical Thinking


The last important point is to think critically about what you read and hear. You need to ensure that the knowledge in your portfolio is accurate and unswayed by either vendor or media hype. Beware of the zealots who insist that their dogma provides the only answer—it may or may not be applicable to you and your project.

Never underestimate the power of commercialism. Just because a web search engine lists a hit first doesn’t mean that it’s the best match; the content provider can pay to get top billing. Just because a bookstore features a book prominently doesn’t mean it’s a good book, or even popular; they may have been paid to place it there.

Tip 10 Critically Analyze What You Read and Hear

Critical thinking is an entire discipline unto itself, and we encourage you to read and study all you can about it. In the meantime, here’s a head start with a few questions to ask and think about.

Ask the “Five Whys”

A favorite consulting trick: ask “why?” at least five times. Ask a question, and get an answer. Dig deeper by asking “why?” Repeat as if you were a petulant four-year old (but a polite one). You might be able to get closer to a root cause this way.


Who does this benefit?

It may sound cynical, but follow the money can be a very helpful path to analyze. The benefits to someone else or another organization may be aligned with your own, or not.


What’s the context?

Everything occurs in its own context, which is why “one size fits all” solutions often don’t. Consider an article or book touting a “best practice.” Good questions to consider are “best for who?” What are the prerequisites, what are the consequences, short and long term?


When or Where would this work?

Under what circumstances? Is it too late? Too early? Don’t stop with first-order thinking (what will happen next), but use second-order thinking: what will happen after that?


Why is this a problem?

Is there an underlying model? How does the underlying model work?


Unfortunately, there are very few simple answers anymore. But with your extensive portfolio, and by applying some critical analysis to the torrent of technical articles you will read, you can understand the complex answers.


Related Sections Include


Topic 1, ​It’s Your Life​

Topic 22, ​Engineering Daybooks​


Challenges


Start learning a new language this week. Always programmed in the same old language? Try Clojure, Elixir, Elm, F#, Go, Haskell, Python, R, ReasonML, Ruby, Rust, Scala, Swift, TypeScript, or anything else that appeals and/or looks as if you might like it.[11]


Start reading a new book (but finish this one first!). If you are doing very detailed implementation and coding, read a book on design and architecture. If you are doing high-level design, read a book on coding techniques.


Get out and talk technology with people who aren’t involved in your current project, or who don’t work for the same company. Network in your company cafeteria, or maybe seek out fellow enthusiasts at a local meetup.


Topic 7

Communicate!


I believe that it is better to be looked over than it is to be overlooked.


Mae West, Belle of the Nineties, 1934


Maybe we can learn a lesson from Ms. West. It’s not just what you’ve got, but also how you package it. Having the best ideas, the finest code, or the most pragmatic thinking is ultimately sterile unless you can communicate with other people. A good idea is an orphan without effective communication.

As developers, we have to communicate on many levels. We spend hours in meetings, listening and talking. We work with end users, trying to understand their needs. We write code, which communicates our intentions to a machine and documents our thinking for future generations of developers. We write proposals and memos requesting and justifying resources, reporting our status, and suggesting new approaches. And we work daily within our teams to advocate our ideas, modify existing practices, and suggest new ones. A large part of our day is spent communicating, so we need to do it well.

Treat English (or whatever your native tongue may be) as just another programming language. Write natural language as you would write code: honor the DRY principle, ETC, automation, and so on. (We discuss the DRY and ETC design principles in the next chapter.)

Tip 11 English is Just Another Programming Language

We’ve put together a list of additional ideas that we find useful.


Know Your Audience


You’re communicating only if you’re conveying what you mean to convey—just talking isn’t enough. To do that, you need to understand the needs, interests, and capabilities of your audience. We’ve all sat in meetings where a development geek glazes over the eyes of the vice president of marketing with a long monologue on the merits of some arcane technology. This isn’t communicating: it’s just talking, and it’s annoying.[12]

Say you want to change your remote monitoring system to use a third-party message broker to disseminate status notifications. You can present this update in many different ways, depending on your audience. End users will appreciate that their systems can now interoperate with other services that use the broker. Your marketing department will be able to use this fact to boost sales. Development and operations managers will be happy because the care and maintenance of that part of the system is now someone else’s problem. Finally, developers may enjoy getting experience with new APIs, and may even be able to find new uses for the message broker. By making the appropriate pitch to each group, you’ll get them all excited about your project.

As with all forms of communication, the trick here is to gather feedback. Don’t just wait for questions: ask for them. Look at body language, and facial expressions. One of the Neuro Linguistic Programming presuppositions is “The meaning of your communication is the response you get.” Continuously improve your knowledge of your audience as you communicate.


Know What You Want to Say


Probably the most difficult part of the more formal styles of communication used in business is working out exactly what it is you want to say. Fiction writers often plot out their books in detail before they start, but people writing technical documents are often happy to sit down at a keyboard, enter:

Introduction


and start typing whatever comes into their heads next.

Plan what you want to say. Write an outline. Then ask yourself, “Does this communicate what I want to express to my audience in a way that works for them?” Refine it until it does.

This approach works for more than just documents. When you’re faced with an important meeting or a chat with a major client, jot down the ideas you want to communicate, and plan a couple of strategies for getting them across.

Now that you know what your audience wants, let’s deliver it.


Choose Your Moment


It’s six o’clock on Friday afternoon, following a week when the auditors have been in. Your boss’s youngest is in the hospital, it’s pouring rain outside, and the commute home is guaranteed to be a nightmare. This probably isn’t a good time to ask her for a memory upgrade for your laptop.

As part of understanding what your audience needs to hear, you need to work out what their priorities are. Catch a manager who’s just been given a hard time by her boss because some source code got lost, and you’ll have a more receptive listener to your ideas on source code repositories. Make what you’re saying relevant in time, as well as in content. Sometimes all it takes is the simple question, “Is this a good time to talk about…?’’


Choose a Style


Adjust the style of your delivery to suit your audience. Some people want a formal “just the facts’’ briefing. Others like a long, wide-ranging chat before getting down to business. What is their skill level and experience in this area? Are they experts? Newbies? Do they need hand-holding or just a quick tl;dr? If in doubt, ask.

Remember, however, that you are half of the communication transaction. If someone says they need a paragraph describing something and you can’t see any way of doing it in less than several pages, tell them so. Remember, that kind of feedback is a form of communication, too.


Make It Look Good


Your ideas are important. They deserve a good-looking vehicle to convey them to your audience.

Too many developers (and their managers) concentrate solely on content when producing written documents. We think this is a mistake. Any chef (or watcher of the Food Network) will tell you that you can slave in the kitchen for hours only to ruin your efforts with poor presentation.

There is no excuse today for producing poor-looking printed documents. Modern software can produce stunning output, regardless of whether you’re writing using Markdown or using a word processor. You need to learn just a few basic commands. If you’re using a word processor, use its style sheets for consistency. (Your company may already have defined style sheets that you can use.) Learn how to set page headers and footers. Look at the sample documents included with your package to get ideas on style and layout. Check the spelling, first automatically and then by hand. After awl, their are spelling miss steaks that the chequer can knot ketch.


Involve Your Audience


We often find that the documents we produce end up being less important than the process we go through to produce them. If possible, involve your readers with early drafts of your document. Get their feedback, and pick their brains. You’ll build a good working relationship, and you’ll probably produce a better document in the process.


Be a Listener


There’s one technique that you must use if you want people to listen to you: listen to them. Even if this is a situation where you have all the information, even if this is a formal meeting with you standing in front of 20 suits—if you don’t listen to them, they won’t listen to you.

Encourage people to talk by asking questions, or ask them to restate the discussion in their own words. Turn the meeting into a dialog, and you’ll make your point more effectively. Who knows, you might even learn something.


Get Back to People


If you ask someone a question, you feel they’re impolite if they don’t respond. But how often do you fail to get back to people when they send you an email or a memo asking for information or requesting some action? In the rush of everyday life, it’s easy to forget. Always respond to emails and voicemails, even if the response is simply “I’ll get back to you later.’’ Keeping people informed makes them far more forgiving of the occasional slip, and makes them feel that you haven’t forgotten them.

Tip 12 It’s Both What You Say and the Way You Say It

Unless you work in a vacuum, you need to be able to communicate. The more effective that communication, the more influential you become.


Documentation


Finally, there’s the matter of communicating via documentation. Typically, developers don’t give much thought to documentation. At best it is an unfortunate necessity; at worst it is treated as a low-priority task in the hope that management will forget about it at the end of the project.

Pragmatic Programmers embrace documentation as an integral part of the overall development process. Writing documentation can be made easier by not duplicating effort or wasting time, and by keeping documentation close at hand—in the code itself. In fact, we want to apply all of our pragmatic principles to documentation as well as to code.

Tip 13 Build Documentation In, Don’t Bolt It On

It’s easy to produce good-looking documentation from the comments in source code, and we recommend adding comments to modules and exported functions to give other developers a leg up when they come to use it.

However, this doesn’t mean we agree with the folks who say that every function, data structure, type declaration, etc., needs its own comment. This kind of mechanical comment writing actually makes it more difficult to maintain code: now there are two things to update when you make a change. So restrict your non-API commenting to discussing why something is done, its purpose and its goal. The code already shows how it is done, so commenting on this is redundant—and is a violation of the DRY principle.

Commenting source code gives you the perfect opportunity to document those elusive bits of a project that can’t be documented anywhere else: engineering trade-offs, why decisions were made, what other alternatives were discarded, and so on.


Summary


Know what you want to say.

Know your audience.

Choose your moment.

Choose a style.

Make it look good.

Involve your audience.

Be a listener.

Get back to people.

Keep code and documentation together.


Related Sections Include


Topic 15, ​Estimating​

Topic 18, ​Power Editing​

Topic 45, ​The Requirements Pit​

Topic 49, ​Pragmatic Teams​


Online Communication

Everything we’ve said about communicating in writing applies equally to email, social media posts, blogs, and so on. Email in particular has evolved to the point where it is a mainstay of corporate communications; it’s used to discuss contracts, to settle disputes, and as evidence in court. But for some reason, people who would never send out a shabby paper document are happy to fling nasty-looking, incoherent emails around the world.

Our tips are simple:

Proofread before you hit SEND.


Check your spelling and look for any accidental auto-correct mishaps.


Keep the format simple and clear.


Keep quoting to a minimum. No one likes to receive back their own 100-line email with “I agree” tacked on.


If you’re quoting other people’s email, be sure to attribute it, and quote it inline (rather than as an attachment). Same when quoting on social media platforms.


Don’t flame or act like a troll unless you want it to come back and haunt you later. If you wouldn’t say it to someone’s face, don’t say it online.


Check your list of recipients before sending. It’s become a cliché to criticize the boss over departmental email without realizing that the boss is on the cc list. Better yet, don’t criticize the boss over email.


As countless large corporations and politicians have discovered, email and social media posts are forever. Try to give the same attention and care to email as you would to any written memo or report.


Challenges


There are several good books that contain sections on communications within teams, including The Mythical Man-Month: Essays on Software Engineering [Bro96] and Peopleware: Productive Projects and Teams [DL13]. Make it a point to try to read these over the next 18 months. In addition, Dinosaur Brains: Dealing with All Those Impossible People at Work [BR89] discusses the emotional baggage we all bring to the work environment.


The next time you have to give a presentation, or write a memo advocating some position, try working through the advice in this section before you start. Explicitly identify the audience and what you need to communicate. If appropriate, talk to your audience afterward and see how accurate your assessment of their needs was.


Footnotes


[3]

http://wiki.c2.com/?ChangeYourOrganization


[4]

See, for example, a good meta-analysis at Trust and team performance: A meta-analysis of main effects, moderators, and covariates, http://dx.doi.org/10.1037/apl0000110


[5]

See The police and neighborhood safety [WH82]


[6]

See Contagious depression: Existence, specificity to depressed symptoms, and the role of reassurance seeking [Joi94]


[7]

While doing this, you may be comforted by the line attributed to Rear Admiral Dr. Grace Hopper: “It’s easier to ask forgiveness than it is to get permission.’’


[8]

That was supposed to be a joke!


[9]

An expiring asset is something whose value diminishes over time. Examples include a warehouse full of bananas and a ticket to a ball game.


[10]

We may be biased, but there’s a fine selection available at https://pragprog.com.


[11]

Never heard of any of these languages? Remember, knowledge is an expiring asset, and so is popular technology. The list of hot new and experimental languages was very different for the first edition, and is probably different again by the time you read this. All the more reason to keep learning.


[12]

The word annoy comes from the Old French enui, which also means “to bore.’’





Chapter 2


A Pragmatic Approach


There are certain tips and tricks that apply at all levels of software development, processes that are virtually universal, and ideas that are almost axiomatic. However, these approaches are rarely documented as such; you’ll mostly find them written down as odd sentences in discussions of design, project management, or coding. But for your convenience, we’ll bring these ideas and processes together here.

The first and maybe most important topic gets to the heart of software development: Topic 8, ​The Essence of Good Design​. Everything follows from this.

The next two sections, Topic 9, ​DRY—The Evils of Duplication​ and Topic 10, ​Orthogonality​, are closely related. The first warns you not to duplicate knowledge throughout your systems, the second not to split any one piece of knowledge across multiple system components.

As the pace of change increases, it becomes harder and harder to keep our applications relevant. In Topic 11, ​Reversibility​, we’ll look at some techniques that help insulate your projects from their changing environment.

The next two sections are also related. In Topic 12, ​Tracer Bullets​, we talk about a style of development that allows you to gather requirements, test designs, and implement code at the same time. It’s the only way to keep up with the pace of modern life.

Topic 13, ​Prototypes and Post-it Notes​ shows you how to use prototyping to test architectures, algorithms, interfaces, and ideas. In the modern world, it’s critical to test ideas and get feedback before you commit to them whole-heartedly.

As computer science slowly matures, designers are producing increasingly higher-level languages. While the compiler that accepts “make it so” hasn’t yet been invented, in Topic 14, ​Domain Languages​ we present some more modest suggestions that you can implement for yourself.

Finally, we all work in a world of limited time and resources. You can survive these scarcities better (and keep your bosses or clients happier) if you get good at working out how long things will take, which we cover in Topic 15, ​Estimating​.

Keep these fundamental principles in mind during development, and you’ll write code that’s better, faster, and stronger. You can even make it look easy.


Topic 8

The Essence of Good Design


The world is full of gurus and pundits, all eager to pass on their hard-earned wisdom when it comes to How to Design Software. There are acronyms, lists (which seem to favor five entries), patterns, diagrams, videos, talks, and (the internet being the internet) probably a cool series on the Law of Demeter explained using interpretive dance.

And we, your gentle authors, are guilty of this too. But we’d like to make amends by explaining something that only became apparent to us fairly recently. First, the general statement:

Tip 14 Good Design Is Easier to Change Than Bad Design

A thing is well designed if it adapts to the people who use it. For code, that means it must adapt by changing. So we believe in the ETC principle: Easier to Change. ETC. That’s it.

As far as we can tell, every design principle out there is a special case of ETC.

Why is decoupling good? Because by isolating concerns we make each easier to change. ETC.

Why is the single responsibility principle useful? Because a change in requirements is mirrored by a change in just one module. ETC.

Why is naming important? Because good names make code easier to read, and you have to read it to change it. ETC!


ETC Is a Value, Not a Rule


Values are things that help you make decisions: should I do this, or that? When it comes to thinking about software, ETC is a guide, helping you choose between paths. Just like all your other values, it should be floating just behind your conscious thought, subtly nudging you in the right direction.

But how do you make that happen? Our experience is that it requires some initial conscious reinforcement. You may need to spend a week or so deliberately asking yourself “did the thing I just did make the overall system easier or harder to change?” Do it when you save a file. Do it when you write a test. Do it when you fix a bug.

There’s an implicit premise in ETC. It assumes that a person can tell which of many paths will be easier to change in the future. Much of the time, common sense will be correct, and you can make an educated guess.

Sometimes, though, you won’t have a clue. That’s OK. In those cases, we think you can do two things.

First, given that you’re not sure what form change will take, you can always fall back on the ultimate “easy to change” path: try to make what you write replaceable. That way, whatever happens in the future, this chunk of code won’t be a roadblock. It seems extreme, but actually it’s what you should be doing all the time, anyway. It’s really just thinking about keeping code decoupled and cohesive.

Second, treat this as a way to develop instincts. Note the situation in your engineering day book: the choices you have, and some guesses about change. Leave a tag in the source. Then, later, when this code has to change, you’ll be able to look back and give yourself feedback. It might help the next time you reach a similar fork in the road.

The rest of the sections in this chapter have specific ideas on design, but all are motivated by this one principle.


Related Sections Include


Topic 9, ​DRY—The Evils of Duplication​

Topic 10, ​Orthogonality​

Topic 11, ​Reversibility​

Topic 14, ​Domain Languages​

Topic 28, ​Decoupling​

Topic 30, ​Transforming Programming​

Topic 31, ​Inheritance Tax​


Challenges


Think about a design principle you use regularly. Is it intended to make things easy-to-change?


Also think about languages and programming paradigms (OO, FP, Reactive, and so on). Do any have either big positives or big negatives when it comes to helping you write ETC code? Do any have both?

When coding, what can you do to eliminate the negatives and accentuate the positives?[13]


Many editors have support (either built-in or via extensions) to run commands when you save a file. Get your editor to popup an ETC? message every time you save[14] and use it as a cue to think about the code you just wrote. Is it easy to change?


Topic 9

DRY—The Evils of Duplication


Giving a computer two contradictory pieces of knowledge was Captain James T. Kirk’s preferred way of disabling a marauding artificial intelligence. Unfortunately, the same principle can be effective in bringing down your code.

As programmers, we collect, organize, maintain, and harness knowledge. We document knowledge in specifications, we make it come alive in running code, and we use it to provide the checks needed during testing.

Unfortunately, knowledge isn’t stable. It changes—often rapidly. Your understanding of a requirement may change following a meeting with the client. The government changes a regulation and some business logic gets outdated. Tests may show that the chosen algorithm won’t work. All this instability means that we spend a large part of our time in maintenance mode, reorganizing and reexpressing the knowledge in our systems.

Most people assume that maintenance begins when an application is released, that maintenance means fixing bugs and enhancing features. We think these people are wrong. Programmers are constantly in maintenance mode. Our understanding changes day by day. New requirements arrive and existing requirements evolve as we’re heads-down on the project. Perhaps the environment changes. Whatever the reason, maintenance is not a discrete activity, but a routine part of the entire development process.

When we perform maintenance, we have to find and change the representations of things—those capsules of knowledge embedded in the application. The problem is that it’s easy to duplicate knowledge in the specifications, processes, and programs that we develop, and when we do so, we invite a maintenance nightmare—one that starts well before the application ships.

We feel that the only way to develop software reliably, and to make our developments easier to understand and maintain, is to follow what we call the DRY principle:

Every piece of knowledge must have a single, unambiguous, authoritative representation within a system.

Why do we call it DRY?

Tip 15 DRY—Don’t Repeat Yourself

The alternative is to have the same thing expressed in two or more places. If you change one, you have to remember to change the others, or, like the alien computers, your program will be brought to its knees by a contradiction. It isn’t a question of whether you’ll remember: it’s a question of when you’ll forget.

You’ll find the DRY principle popping up time and time again throughout this book, often in contexts that have nothing to do with coding. We feel that it is one of the most important tools in the Pragmatic Programmer’s tool box.

In this section we’ll outline the problems of duplication and suggest general strategies for dealing with it.


DRY Is More Than Code


Let’s get something out of the way up-front. In the first edition of this book we did a poor job of explaining just what we meant by Don’t Repeat Yourself. Many people took it to refer to code only: they thought that DRY means “don’t copy-and-paste lines of source.”

That is part of DRY, but it’s a tiny and fairly trivial part.

DRY is about the duplication of knowledge, of intent. It’s about expressing the same thing in two different places, possibly in two totally different ways.

Here’s the acid test: when some single facet of the code has to change, do you find yourself making that change in multiple places, and in multiple different formats? Do you have to change code and documentation, or a database schema and a structure that holds it, or…? If so, your code isn’t DRY.

So let’s look at some typical examples of duplication.


Duplication in Code


It may be trivial, but code duplication is oh, so common. Here’s an example:

​ ​def​ print_balance(account)

​ printf ​"Debits: %10.2f​​\n​​"​, account.debits

​ printf ​"Credits: %10.2f​​\n​​"​, account.credits

​ ​if​ account.fees < 0

​ printf ​"Fees: %10.2f-​​\n​​"​, -account.fees

​ ​else​

​ printf ​"Fees: %10.2f​​\n​​"​, account.fees

​ ​end​

​ printf ​" ———-​​\n​​"​

​ ​if​ account.balance < 0

​ printf ​"Balance: %10.2f-​​\n​​"​, -account.balance

​ ​else​

​ printf ​"Balance: %10.2f​​\n​​"​, account.balance

​ ​end​

​ ​end​

For now ignore the implication that we’re committing the newbie mistake of storing currencies in floats. Instead see if you can spot duplications in this code. (We can see at least three things, but you might see more.)

What did you find? Here’s our list.

First, there’s clearly a copy-and-paste duplication of handling the negative numbers. We can fix that by adding another function:

​ ​def​ format_amount(value)

​ result = sprintf(​"%10.2f"​, value.abs)

​ ​if​ value < 0

​ result + ​"-"​

​ ​else​

​ result + ​" "​

​ ​end​

​ ​end​

​

​ ​def​ print_balance(account)

​ printf ​"Debits: %10.2f​​\n​​"​, account.debits

​ printf ​"Credits: %10.2f​​\n​​"​, account.credits

​ printf ​"Fees: %s​​\n​​"​, format_amount(account.fees)

​ printf ​" ———-​​\n​​"​

​ printf ​"Balance: %s​​\n​​"​, format_amount(account.balance)

​ ​end​

Another duplication is the repetition of the field width in all the printf calls. We could fix this by introducing a constant and passing it to each call, but why not just use the existing function?

​ ​def​ format_amount(value)

​ result = sprintf(​"%10.2f"​, value.abs)

​ ​if​ value < 0

​ result + ​"-"​

​ ​else​

​ result + ​" "​

​ ​end​

​ ​end​

​

​ ​def​ print_balance(account)

​ printf ​"Debits: %s​​\n​​"​, format_amount(account.debits)

​ printf ​"Credits: %s​​\n​​"​, format_amount(account.credits)

​ printf ​"Fees: %s​​\n​​"​, format_amount(account.fees)

​ printf ​" ———-​​\n​​"​

​ printf ​"Balance: %s​​\n​​"​, format_amount(account.balance)

​ ​end​

Anything more? Well, what if the client asks for an extra space between the labels and the numbers? We’d have to change five lines. Let’s remove that duplication:

​ ​def​ format_amount(value)

​ result = sprintf(​"%10.2f"​, value.abs)

​ ​if​ value < 0

​ result + ​"-"​

​ ​else​

​ result + ​" "​

​ ​end​

​ ​end​

​

​ ​def​ print_line(label, value)

​ printf ​"%-9s%s​​\n​​"​, label, value

​ ​end​

​

​ ​def​ report_line(label, amount)

​ print_line(label + ​":"​, format_amount(amount))

​ ​end​

​

​ ​def​ print_balance(account)

​ report_line(​"Debits"​, account.debits)

​ report_line(​"Credits"​, account.credits)

​ report_line(​"Fees"​, account.fees)

​ print_line(​""​, ​"———-"​)

​ report_line(​"Balance"​, account.balance)

​ ​end​

If we have to change the formatting of amounts, we change format_amount. If we want to change the label format, we change report_line.

There’s still an implicit DRY violation: the number of hyphens in the separator line is related to the width of the amount field. But it isn’t an exact match: it’s currently one character shorter, so any trailing minus signs extend beyond the column. This is the customer’s intent, and it’s a different intent to the actual formatting of amounts.


Not All Code Duplication Is Knowledge Duplication


As part of your online wine ordering application you’re capturing and validating your user’s age, along with the quantity they’re ordering. According to the site owner, they should both be numbers, and both greater than zero. So you code up the validations:

​ ​def​ validate_age(value):

​ validate_type(value, :integer)

​ validate_min_integer(value, 0)

​

​ ​def​ validate_quantity(value):

​ validate_type(value, :integer)

​ validate_min_integer(value, 0)

During code review, the resident know-it-all bounces this code, claiming it’s a DRY violation: both function bodies are the same.

They are wrong. The code is the same, but the knowledge they represent is different. The two functions validate two separate things that just happen to have the same rules. That’s a coincidence, not a duplication.


Duplication in Documentation


Somehow the myth was born that you should comment all your functions. Those who believe in this insanity then produce something such as this:

​ ​# Calculate the fees for this account.​

​ ​#​

​ ​# * Each returned check costs $20​

​ ​# * If the account is in overdraft for more than 3 days,​

​ ​# charge $10 for each day​

​ ​# * If the average account balance is greater that $2,000​

​ ​# reduce the fees by 50%​

​

​ ​def​ fees(a)

​ f = 0

​ ​if​ a.returned_check_count > 0

​ f += 20 * a.returned_check_count

​ ​end​

​ ​if​ a.overdraft_days > 3

​ f += 10*a.overdraft_days

​ ​end​

​ ​if​ a.average_balance > 2_000

​ f /= 2

​ ​end​

​ f

​ ​end​

The intent of this function is given twice: once in the comment and again in the code. The customer changes a fee, and we have to update both. Given time, we can pretty much guarantee the comment and the code will get out of step.

Ask yourself what the comment adds to the code. From our point of view, it simply compensates for some bad naming and layout. How about just this:

​ ​def​ calculate_account_fees(account)

​ fees = 20 * account.returned_check_count

​ fees += 10 * account.overdraft_days ​if​ account.overdraft_days > 3

​ fees /= 2 ​if​ account.average_balance > 2_000

​ fees

​ ​end​

The name says what it does, and if someone needs details, they’re laid out in the source. That’s DRY!


DRY Violations in Data


Our data structures represent knowledge, and they can fall afoul of the DRY principle. Let’s look at a class representing a line:

​ ​class​ Line {

​ Point start;

​ Point end;

​ ​double​ length;

​ };

At first sight, this class might appear reasonable. A line clearly has a start and end, and will always have a length (even if it’s zero). But we have duplication. The length is defined by the start and end points: change one of the points and the length changes. It’s better to make the length a calculated field:

​ ​class​ Line {

​ Point start;

​ Point end;

​ ​double​ length() { ​return​ start.distanceTo(end); }

​ };

Later on in the development process, you may choose to violate the DRY principle for performance reasons. Frequently this occurs when you need to cache data to avoid repeating expensive operations. The trick is to localize the impact. The violation is not exposed to the outside world: only the methods within the class have to worry about keeping things straight:

​ ​class​ Line {

​ ​private​ ​double​ length;

​ ​private​ Point start;

​ ​private​ Point end;

​

​ ​public​ Line(Point start, Point end) {

​ ​this​.start = start;

​ ​this​.end = end;

​ calculateLength();

​ }

​

​ ​// public​

​ ​void​ setStart(Point p) { ​this​.start = p; calculateLength(); }

​ ​void​ setEnd(Point p) { ​this​.end = p; calculateLength(); }

​

​ Point getStart() { ​return​ start; }

​ Point getEnd() { ​return​ end; }

​

​ ​double​ getLength() { ​return​ length; }

​

​ ​private​ ​void​ calculateLength() {

​ ​this​.length = start.distanceTo(end);

​ }

​ };

This example also illustrates an important issue: whenever a module exposes a data structure, you’re coupling all the code that uses that structure to the implementation of that module. Where possible, always use accessor functions to read and write the attributes of objects. It will make it easier to add functionality in the future.

This use of accessor functions ties in with Meyer’s Uniform Access principle, described in Object-Oriented Software Construction [Mey97], which states that

All services offered by a module should be available through a uniform notation, which does not betray whether they are implemented through storage or through computation.


Representational Duplication


Your code interfaces to the outside world: other libraries via APIs, other services via remote calls, data in external sources, and so on. And pretty much each time you do, you introduce some kind of DRY violation: your code has to have knowledge that is also present in the external thing. It needs to know the API, or the schema, or the meaning of error codes, or whatever. The duplication here is that two things (your code and the external entity) have to have knowledge of the representation of their interface. Change it at one end, and the other end breaks.

This duplication is inevitable, but can be mitigated. Here are some strategies.


Duplication Across Internal APIs


For internal APIs, look for tools that let you specify the API in some kind of neutral format. These tools will typically generate documentation, mock APIs, functional tests, and API clients, the latter in a number of different languages. Ideally the tool will store all your APIs in a central repository, allowing them to be shared across teams.


Duplication Across External APIs


Increasingly, you’ll find that public APIs are documented formally using something like OpenAPI.[15] This allows you to import the API spec into your local API tools and integrate more reliably with the service.

If you can’t find such a specification, consider creating one and publishing it. Not only will others find it useful; you may even get help maintaining it.


Duplication with Data Sources


Many data sources allow you to introspect on their data schema. This can be used to remove much of the duplication between them and your code. Rather than manually creating the code to contain this stored data, you can generate the containers directly from the schema. Many persistence frameworks will do this heavy lifting for you.

There’s another option, and one we often prefer. Rather than writing code that represents external data in a fixed structure (an instance of a struct or class, for example), just stick it into a key/value data structure (your language might call it a map, hash, dictionary, or even object).

On its own this is risky: you lose a lot of the security of knowing just what data you’re working with. So we recommend adding a second layer to this solution: a simple table-driven validation suite that verifies that the map you’ve created contains at least the data you need, in the format you need it. Your API documentation tool might be able to generate this.


Interdeveloper Duplication


Perhaps the hardest type of duplication to detect and handle occurs between different developers on a project. Entire sets of functionality may be inadvertently duplicated, and that duplication could go undetected for years, leading to maintenance problems. We heard firsthand of a U.S. state whose governmental computer systems were surveyed for Y2K compliance. The audit turned up more than 10,000 programs that each contained a different version of Social Security Number validation code.

At a high level, deal with the problem by building a strong, tight-knit team with good communications.

However, at the module level, the problem is more insidious. Commonly needed functionality or data that doesn’t fall into an obvious area of responsibility can get implemented many times over.

We feel that the best way to deal with this is to encourage active and frequent communication between developers.

Maybe run a daily scrum standup meeting. Set up forums (such as Slack channels) to discuss common problems. This provides a nonintrusive way of communicating—even across multiple sites—while retaining a permanent history of everything said.

Appoint a team member as the project librarian, whose job is to facilitate the exchange of knowledge. Have a central place in the source tree where utility routines and scripts can be deposited. And make a point of reading other people’s source code and documentation, either informally or during code reviews. You’re not snooping—you’re learning from them. And remember, the access is reciprocal—don’t get twisted about other people poring (pawing?) through your code, either.

Tip 16 Make It Easy to Reuse

What you’re trying to do is foster an environment where it’s easier to find and reuse existing stuff than to write it yourself. If it isn’t easy, people won’t do it. And if you fail to reuse, you risk duplicating knowledge.


Related Sections Include


Topic 8, ​The Essence of Good Design​

Topic 28, ​Decoupling​

Topic 32, ​Configuration​

Topic 38, ​Programming by Coincidence​

Topic 40, ​Refactoring​


Topic 10

Orthogonality


Orthogonality is a critical concept if you want to produce systems that are easy to design, build, test, and extend. However, the concept of orthogonality is rarely taught directly. Often it is an implicit feature of various other methods and techniques you learn. This is a mistake. Once you learn to apply the principle of orthogonality directly, you’ll notice an immediate improvement in the quality of systems you produce.


What Is Orthogonality?


“Orthogonality’’ is a term borrowed from geometry. Two lines are orthogonal if they meet at right angles, such as the axes on a graph. In vector terms, the two lines are independent. As the number 1 on the diagram moves north, it doesn’t change how far east or west it is. The number 2 moves east, but not north or south.

In computing, the term has come to signify a kind of independence or decoupling. Two or more things are orthogonal if changes in one do not affect any of the others. In a well-designed system, the database code will be orthogonal to the user interface: you can change the interface without affecting the database, and swap databases without changing the interface.

Before we look at the benefits of orthogonal systems, let’s first look at a system that isn’t orthogonal.


A Nonorthogonal System


You’re on a helicopter tour of the Grand Canyon when the pilot, who made the obvious mistake of eating fish for lunch, suddenly groans and faints. Fortunately, he left you hovering 100 feet above the ground.

As luck would have it, you had read a Wikipedia page about helicopters the previous night. You know that helicopters have four basic controls. The cyclic is the stick you hold in your right hand. Move it, and the helicopter moves in the corresponding direction. Your left hand holds the collective pitch lever. Pull up on this and you increase the pitch on all the blades, generating lift. At the end of the pitch lever is the throttle. Finally you have two foot pedals, which vary the amount of tail rotor thrust and so help turn the helicopter.

“Easy!,” you think. “Gently lower the collective pitch lever and you’ll descend gracefully to the ground, a hero.” However, when you try it, you discover that life isn’t that simple. The helicopter’s nose drops, and you start to spiral down to the left. Suddenly you discover that you’re flying a system where every control input has secondary effects. Lower the left-hand lever and you need to add compensating backward movement to the right-hand stick and push the right pedal. But then each of these changes affects all of the other controls again. Suddenly you’re juggling an unbelievably complex system, where every change impacts all the other inputs. Your workload is phenomenal: your hands and feet are constantly moving, trying to balance all the interacting forces.

Helicopter controls are decidedly not orthogonal.


Benefits of Orthogonality


As the helicopter example illustrates, nonorthogonal systems are inherently more complex to change and control. When components of any system are highly interdependent, there is no such thing as a local fix.

Tip 17 Eliminate Effects Between Unrelated Things

We want to design components that are self-contained: independent, and with a single, well-defined purpose (what Yourdon and Constantine call cohesion in Structured Design: Fundamentals of a Discipline of Computer Program and Systems Design [YC79]). When components are isolated from one another, you know that you can change one without having to worry about the rest. As long as you don’t change that component’s external interfaces, you can be confident that you won’t cause problems that ripple through the entire system.

You get two major benefits if you write orthogonal systems: increased productivity and reduced risk.


Gain Productivity


Changes are localized, so development time and testing time are reduced. It is easier to write relatively small, self-contained components than a single large block of code. Simple components can be designed, coded, tested, and then forgotten—there is no need to keep changing existing code as you add new code.


An orthogonal approach also promotes reuse. If components have specific, well-defined responsibilities, they can be combined with new components in ways that were not envisioned by their original implementors. The more loosely coupled your systems, the easier they are to reconfigure and reengineer.


There is a fairly subtle gain in productivity when you combine orthogonal components. Assume that one component does distinct things and another does things. If they are orthogonal and you combine them, the result does things. However, if the two components are not orthogonal, there will be overlap, and the result will do less. You get more functionality per unit effort by combining orthogonal components.


Reduce Risk


An orthogonal approach reduces the risks inherent in any development.

Diseased sections of code are isolated. If a module is sick, it is less likely to spread the symptoms around the rest of the system. It is also easier to slice it out and transplant in something new and healthy.


The resulting system is less fragile. Make small changes and fixes to a particular area, and any problems you generate will be restricted to that area.


An orthogonal system will probably be better tested, because it will be easier to design and run tests on its components.


You will not be as tightly tied to a particular vendor, product, or platform, because the interfaces to these third-party components will be isolated to smaller parts of the overall development.


Let’s look at some of the ways you can apply the principle of orthogonality to your work.


Design


Most developers are familiar with the need to design orthogonal systems, although they may use words such as modular, component-based, and layered to describe the process. Systems should be composed of a set of cooperating modules, each of which implements functionality independent of the others. Sometimes these components are organized into layers, each providing a level of abstraction. This layered approach is a powerful way to design orthogonal systems. Because each layer uses only the abstractions provided by the layers below it, you have great flexibility in changing underlying implementations without affecting code. Layering also reduces the risk of runaway dependencies between modules. You’ll often see layering expressed in diagrams:


There is an easy test for orthogonal design. Once you have your components mapped out, ask yourself: If I dramatically change the requirements behind a particular function, how many modules are affected? In an orthogonal system, the answer should be “one.’’[16] Moving a button on a GUI panel should not require a change in the database schema. Adding context-sensitive help should not change the billing subsystem.

Let’s consider a complex system for monitoring and controlling a heating plant. The original requirement called for a graphical user interface, but the requirements were changed to add a mobile interface that lets engineers monitor key values. In an orthogonally designed system, you would need to change only those modules associated with the user interface to handle this: the underlying logic of controlling the plant would remain unchanged. In fact, if you structure your system carefully, you should be able to support both interfaces with the same underlying code base.

Also ask yourself how decoupled your design is from changes in the real world. Are you using a telephone number as a customer identifier? What happens when the phone company reassigns area codes? Postal codes, Social Security Numbers or government IDs, email addresses, and domains are all external identifiers that you have no control over, and could change at any time for any reason. Don’t rely on the properties of things you can’t control.


Toolkits and Libraries


Be careful to preserve the orthogonality of your system as you introduce third-party toolkits and libraries. Choose your technologies wisely.

When you bring in a toolkit (or even a library from other members of your team), ask yourself whether it imposes changes on your code that shouldn’t be there. If an object persistence scheme is transparent, then it’s orthogonal. If it requires you to create or access objects in a special way, then it’s not. Keeping such details isolated from your code has the added benefit of making it easier to change vendors in the future.

The Enterprise Java Beans (EJB) system is an interesting example of orthogonality. In most transaction-oriented systems, the application code has to delineate the start and end of each transaction. With EJB, this information is expressed declaratively as annotations, outside the methods that do the work. The same application code can run in different EJB transaction environments with no change.

In a way, EJB is an example of the Decorator Pattern: adding functionality to things without changing them. This style of programming can be used in just about every programming language, and doesn’t necessarily require a framework or library. It just takes a little discipline when programming.


Coding


Every time you write code you run the risk of reducing the orthogonality of your application. Unless you constantly monitor not just what you are doing but also the larger context of the application, you might unintentionally duplicate functionality in some other module, or express existing knowledge twice.

There are several techniques you can use to maintain orthogonality:

Keep your code decoupled

Write shy code—modules that don’t reveal anything unnecessary to other modules and that don’t rely on other modules’ implementations. Try the Law of Demeter, which we discuss in Topic 28, ​Decoupling​. If you need to change an object’s state, get the object to do it for you. This way your code remains isolated from the other code’s implementation and increases the chances that you’ll remain orthogonal.


Avoid global data

Every time your code references global data, it ties itself into the other components that share that data. Even globals that you intend only to read can lead to trouble (for example, if you suddenly need to change your code to be multithreaded). In general, your code is easier to understand and maintain if you explicitly pass any required context into your modules. In object-oriented applications, context is often passed as parameters to objects’ constructors. In other code, you can create structures containing the context and pass around references to them.


The Singleton pattern in Design Patterns: Elements of Reusable Object-Oriented Software [GHJV95] is a way of ensuring that there is only one instance of an object of a particular class. Many people use these singleton objects as a kind of global variable (particularly in languages, such as Java, that otherwise do not support the concept of globals). Be careful with singletons—they can also lead to unnecessary linkage.


Avoid similar functions

Often you’ll come across a set of functions that all look similar—maybe they share common code at the start and end, but each has a different central algorithm. Duplicate code is a symptom of structural problems. Have a look at the Strategy pattern in Design Patterns for a better implementation.


Get into the habit of being constantly critical of your code. Look for any opportunities to reorganize it to improve its structure and orthogonality. This process is called refactoring, and it’s so important that we’ve dedicated a section to it (see Topic 40, ​Refactoring​).


Testing


An orthogonally designed and implemented system is easier to test. Because the interactions between the system’s components are formalized and limited, more of the system testing can be performed at the individual module level. This is good news, because module level (or unit) testing is considerably easier to specify and perform than integration testing. In fact, we suggest that these tests be performed automatically as part of the regular build process (see Topic 41, ​Test to Code​).

Writing unit tests is itself an interesting test of orthogonality. What does it take to get a unit test to build and run? Do you have to import a large percentage of the rest of the system’s code? If so, you’ve found a module that is not well decoupled from the rest of the system.

Bug fixing is also a good time to assess the orthogonality of the system as a whole. When you come across a problem, assess how localized the fix is. Do you change just one module, or are the changes scattered throughout the entire system? When you make a change, does it fix everything, or do other problems mysteriously arise? This is a good opportunity to bring automation to bear. If you use a version control system (and you will after reading Topic 19, ​Version Control​), tag bug fixes when you check the code back in after testing. You can then run monthly reports analyzing trends in the number of source files affected by each bug fix.


Documentation


Perhaps surprisingly, orthogonality also applies to documentation. The axes are content and presentation. With truly orthogonal documentation, you should be able to change the appearance dramatically without changing the content. Word processors provide style sheets and macros that help. We personally prefer using a markup system such as Markdown: when writing we focus only on the content, and leave the presentation to whichever tool we use to render it.[17]


Living with Orthogonality


Orthogonality is closely related to the DRY principle. With DRY, you’re looking to minimize duplication within a system, whereas with orthogonality you reduce the interdependency among the system’s components. It may be a clumsy word, but if you use the principle of orthogonality, combined closely with the DRY principle, you’ll find that the systems you develop are more flexible, more understandable, and easier to debug, test, and maintain.

If you’re brought into a project where people are desperately struggling to make changes, and where every change seems to cause four other things to go wrong, remember the nightmare with the helicopter. The project probably is not orthogonally designed and coded. It’s time to refactor.

And, if you’re a helicopter pilot, don’t eat the fish….


Related Sections Include


Topic 3, ​Software Entropy​

Topic 8, ​The Essence of Good Design​

Topic 11, ​Reversibility​

Topic 28, ​Decoupling​

Topic 31, ​Inheritance Tax​

Topic 33, ​Breaking Temporal Coupling​

Topic 34, ​Shared State Is Incorrect State​

Topic 36, ​Blackboards​


Challenges


Consider the difference between tools which have a graphical user interface and small but combinable command-line utilities used at shell prompts. Which set is more orthogonal, and why? Which is easier to use for exactly the purpose for which it was intended? Which set is easier to combine with other tools to meet new challenges? Which set is easier to learn?


C++ supports multiple inheritance, and Java allows a class to implement multiple interfaces. Ruby has mixins. What impact does using these facilities have on orthogonality? Is there a difference in impact between using multiple inheritance and multiple interfaces? Is there a difference between using delegation and using inheritance?


Exercises


Exercise 1 (possible answer)

You’re asked to read a file a line at a time. For each line, you have to split it into fields. Which of the following sets of pseudo class definitions is likely to be more orthogonal?

​ class Split1 {

​ constructor(fileName) # opens the file for reading

​ def readNextLine() # moves to the next line

​ def getField(n) # returns nth field in current line

​ }

or

​ class Split2 {

​ constructor(line) # splits a line

​ def getField(n) # returns nth field in current line

​ }

Exercise 2 (possible answer)

What are the differences in orthogonality between object-oriented and functional languages? Are these differences inherent in the languages themselves, or just in the way people use them?


Topic 11

Reversibility


Nothing is more dangerous than an idea if it’s the only one you have.


Emil-Auguste Chartier (Alain), Propos sur la religion, 1938


Engineers prefer simple, singular solutions to problems. Math tests that allow you to proclaim with great confidence that are much more comfortable than fuzzy, warm essays about the myriad causes of the French Revolution. Management tends to agree with the engineers: singular, easy answers fit nicely on spreadsheets and project plans.

If only the real world would cooperate! Unfortunately, while is today, it may need to be tomorrow, and next week. Nothing is forever—and if you rely heavily on some fact, you can almost guarantee that it will change.

There is always more than one way to implement something, and there is usually more than one vendor available to provide a third-party product. If you go into a project hampered by the myopic notion that there is only one way to do it, you may be in for an unpleasant surprise. Many project teams have their eyes forcibly opened as the future unfolds:

“But you said we’d use database XYZ! We are 85% done coding the project, we can’t change now!” the programmer protested. “Sorry, but our company decided to standardize on database PDQ instead—for all projects. It’s out of my hands. We’ll just have to recode. All of you will be working weekends until further notice.”


Changes don’t have to be that Draconian, or even that immediate. But as time goes by, and your project progresses, you may find yourself stuck in an untenable position. With every critical decision, the project team commits to a smaller target—a narrower version of reality that has fewer options.

By the time many critical decisions have been made, the target becomes so small that if it moves, or the wind changes direction, or a butterfly in Tokyo flaps its wings, you miss.[18] And you may miss by a huge amount.

The problem is that critical decisions aren’t easily reversible.

Once you decide to use this vendor’s database, or that architectural pattern, or a certain deployment model, you are committed to a course of action that cannot be undone, except at great expense.


Reversibility


Many of the topics in this book are geared to producing flexible, adaptable software. By sticking to their recommendations—especially the DRY principle, decoupling, and use of external configuration—we don’t have to make as many critical, irreversible decisions. This is a good thing, because we don’t always make the best decisions the first time around. We commit to a certain technology only to discover we can’t hire enough people with the necessary skills. We lock in a certain third-party vendor just before they get bought out by their competitor. Requirements, users, and hardware change faster than we can get the software developed.

Suppose you decide, early in the project, to use a relational database from vendor A. Much later, during performance testing, you discover that the database is simply too slow, but that the document database from vendor B is faster. With most conventional projects, you’d be out of luck. Most of the time, calls to third-party products are entangled throughout the code. But if you really abstracted the idea of a database out—to the point where it simply provides persistence as a service—then you have the flexibility to change horses in midstream.

Similarly, suppose the project begins as a browser-based application, but then, late in the game, marketing decides that what they really want is a mobile app. How hard would that be for you? In an ideal world, it shouldn’t impact you too much, at least on the server side. You’d be stripping out some HTML rendering and replacing it with an API.

The mistake lies in assuming that any decision is cast in stone—and in not preparing for the contingencies that might arise. Instead of carving decisions in stone, think of them more as being written in the sand at the beach. A big wave can come along and wipe them out at any time.

Tip 18 There Are No Final Decisions


Flexible Architecture


While many people try to keep their code flexible, you also need to think about maintaining flexibility in the areas of architecture, deployment, and vendor integration.

We’re writing this in 2019. Since the turn of the century we’ve seen the following “best practice” server-side architectures:

Big hunk of iron

Federations of big iron

Load-balanced clusters of commodity hardware

Cloud-based virtual machines running applications

Cloud-based virtual machines running services

Containerized versions of the above

Cloud-supported serverless applications

And, inevitably, an apparent move back to big hunks of iron for some tasks


Go ahead and add the very latest and greatest fads to this list, and then regard it with awe: it’s a miracle that anything ever worked.

How can you plan for this kind of architectural volatility? You can’t.

What you can do is make it easy to change. Hide third-party APIs behind your own abstraction layers. Break your code into components: even if you end up deploying them on a single massive server, this approach is a lot easier than taking a monolithic application and splitting it. (We have the scars to prove it.)

And, although this isn’t particularly a reversibility issue, one final piece of advice.

Tip 19 Forgo Following Fads

No one knows what the future may hold, especially not us! So enable your code to rock-n-roll: to “rock on’’ when it can, to roll with the punches when it must.


Related Sections Include


Topic 8, ​The Essence of Good Design​

Topic 10, ​Orthogonality​

Topic 19, ​Version Control​

Topic 28, ​Decoupling​

Topic 45, ​The Requirements Pit​

Topic 51, ​Pragmatic Starter Kit​


Challenges


Time for a little quantum mechanics with Schrödinger’s cat.

Suppose you have a cat in a closed box, along with a radioactive particle. The particle has exactly a 50% chance of fissioning into two. If it does, the cat will be killed. If it doesn’t, the cat will be okay. So, is the cat dead or alive? According to Schrödinger, the correct answer is both (at least while the box remains closed). Every time a subnuclear reaction takes place that has two possible outcomes, the universe is cloned. In one, the event occurred, in the other it didn’t. The cat’s alive in one universe, dead in another. Only when you open the box do you know which universe you are in.

No wonder coding for the future is difficult.

But think of code evolution along the same lines as a box full of Schrödinger’s cats: every decision results in a different version of the future. How many possible futures can your code support? Which ones are more likely? How hard will it be to support them when the time comes?

Dare you open the box?


Topic 12

Tracer Bullets


Ready, fire, aim…


Anon


We often talk about hitting targets when we develop software. We’re not actually firing anything at the shooting range, but it’s still a useful and very visual metaphor. In particular, it’s interesting to consider how to hit a target in a complex and shifting world.

The answer, of course, depends on the nature of the device you’re aiming with. With many you only get one chance to aim, and then get to see if you hit the bullseye or not. But there’s a better way.

You know all those movies, TV shows, and video games where people are shooting machine guns? In these scenes, you’ll often see the path of bullets as bright streaks in the air. These streaks come from tracer bullets.

Tracer bullets are loaded at intervals alongside regular ammunition. When they’re fired, their phosphorus ignites and leaves a pyrotechnic trail from the gun to whatever they hit. If the tracers are hitting the target, then so are the regular bullets. Soldiers use these tracer rounds to refine their aim: it’s pragmatic, real-time feedback under actual conditions.

That same principle applies to projects, particularly when you’re building something that hasn’t been built before. We use the term tracer bullet development to visually illustrate the need for immediate feedback under actual conditions with a moving goal.

Like the gunners, you’re trying to hit a target in the dark. Because your users have never seen a system like this before, their requirements may be vague. Because you may be using algorithms, techniques, languages, or libraries you aren’t familiar with, you face a large number of unknowns. And because projects take time to complete, you can pretty much guarantee the environment you’re working in will change before you’re done.

The classic response is to specify the system to death. Produce reams of paper itemizing every requirement, tying down every unknown, and constraining the environment. Fire the gun using dead reckoning. One big calculation up front, then shoot and hope.

Pragmatic Programmers, however, tend to prefer using the software equivalent of tracer bullets.


Code That Glows in the Dark


Tracer bullets work because they operate in the same environment and under the same constraints as the real bullets. They get to the target fast, so the gunner gets immediate feedback. And from a practical standpoint they’re a relatively cheap solution.

To get the same effect in code, we look for something that gets us from a requirement to some aspect of the final system quickly, visibly, and repeatably.

Look for the important requirements, the ones that define the system. Look for the areas where you have doubts, and where you see the biggest risks. Then prioritize your development so that these are the first areas you code.

Tip 20 Use Tracer Bullets to Find the Target

In fact, given the complexity of today’s project setup, with swarms of external dependencies and tools, tracer bullets become even more important. For us, the very first tracer bullet is simply create the project, add a “hello world!,” and make sure it compiles and runs. Then we look for areas of uncertainty in the overall application and add the skeleton needed to make it work.

Have a look at the following diagram. This system has five architectural layers. We have some concerns about how they’d integrate, so we look for a simple feature that lets us exercise them together. The diagonal line shows the path that feature takes through the code. To make it work, we just have to implement the solidly shaded areas in each layer: the stuff with the squiggles will be done later.


We once undertook a complex client-server database marketing project. Part of its requirement was the ability to specify and execute temporal queries. The servers were a range of relational and specialized databases. The client UI, written in random language A, used a set of libraries written in a different language to provide an interface to the servers. The user’s query was stored on the server in a Lisp-like notation before being converted to optimized SQL just prior to execution. There were many unknowns and many different environments, and no one was too sure how the UI should behave.

This was a great opportunity to use tracer code. We developed the framework for the front end, libraries for representing the queries, and a structure for converting a stored query into a database-specific query. Then we put it all together and checked that it worked. For that initial build, all we could do was submit a query that listed all the rows in a table, but it proved that the UI could talk to the libraries, the libraries could serialize and unserialize a query, and the server could generate SQL from the result. Over the following months we gradually fleshed out this basic structure, adding new functionality by augmenting each component of the tracer code in parallel. When the UI added a new query type, the library grew and the SQL generation was made more sophisticated.

Tracer code is not disposable: you write it for keeps. It contains all the error checking, structuring, documentation, and self-checking that any piece of production code has. It simply is not fully functional. However, once you have achieved an end-to-end connection among the components of your system, you can check how close to the target you are, adjusting if necessary. Once you’re on target, adding functionality is easy.

Tracer development is consistent with the idea that a project is never finished: there will always be changes required and functions to add. It is an incremental approach.

The conventional alternative is a kind of heavy engineering approach: code is divided into modules, which are coded in a vacuum. Modules are combined into subassemblies, which are then further combined, until one day you have a complete application. Only then can the application as a whole be presented to the user and tested.

The tracer code approach has many advantages:

Users get to see something working early

If you have successfully communicated what you are doing (see Topic 52, ​Delight Your Users​), your users will know they are seeing something immature. They won’t be disappointed by a lack of functionality; they’ll be ecstatic to see some visible progress toward their system. They also get to contribute as the project progresses, increasing their buy-in. These same users will likely be the people who’ll tell you how close to the target each iteration is.


Developers build a structure to work in

The most daunting piece of paper is the one with nothing written on it. If you have worked out all the end-to-end interactions of your application, and have embodied them in code, then your team won’t need to pull as much out of thin air. This makes everyone more productive, and encourages consistency.


You have an integration platform

As the system is connected end-to-end, you have an environment to which you can add new pieces of code once they have been unit-tested. Rather than attempting a big-bang integration, you’ll be integrating every day (often many times a day). The impact of each new change is more apparent, and the interactions are more limited, so debugging and testing are faster and more accurate.


You have something to demonstrate

Project sponsors and top brass have a tendency to want to see demos at the most inconvenient times. With tracer code, you’ll always have something to show them.


You have a better feel for progress

In a tracer code development, developers tackle use cases one by one. When one is done, they move to the next. It is far easier to measure performance and to demonstrate progress to your user. Because each individual development is smaller, you avoid creating those monolithic blocks of code that are reported as 95% complete week after week.


Tracer Bullets Don’t Always Hit Their Target


Tracer bullets show what you’re hitting. This may not always be the target. You then adjust your aim until they’re on target. That’s the point.

It’s the same with tracer code. You use the technique in situations where you’re not 100% certain of where you’re going. You shouldn’t be surprised if your first couple of attempts miss: the user says “that’s not what I meant,’’ or data you need isn’t available when you need it, or performance problems seem likely. So change what you’ve got to bring it nearer the target, and be thankful that you’ve used a lean development methodology; a small body of code has low inertia—it is easy and quick to change. You’ll be able to gather feedback on your application and generate a new, more accurate version quickly and cheaply. And because every major application component is represented in your tracer code, your users can be confident that what they’re seeing is based on reality, not just a paper specification.


Tracer Code versus Prototyping


You might think that this tracer code concept is nothing more than prototyping under an aggressive name. There is a difference. With a prototype, you’re aiming to explore specific aspects of the final system. With a true prototype, you will throw away whatever you lashed together when trying out the concept, and recode it properly using the lessons you’ve learned.

For example, say you’re producing an application that helps shippers determine how to pack odd-sized boxes into containers. Among other problems, the user interface needs to be intuitive and the algorithms you use to determine optimal packing are very complex.

You could prototype a user interface for your end users in a UI tool. You code only enough to make the interface responsive to user actions. Once they’ve agreed to the layout, you might throw it away and recode it, this time with the business logic behind it, using the target language. Similarly, you might want to prototype a number of algorithms that perform the actual packing. You might code functional tests in a high-level, forgiving language such as Python, and code low-level performance tests in something closer to the machine. In any case, once you’d made your decision, you’d start again and code the algorithms in their final environment, interfacing to the real world. This is prototyping, and it is very useful.

The tracer code approach addresses a different problem. You need to know how the application as a whole hangs together. You want to show your users how the interactions will work in practice, and you want to give your developers an architectural skeleton on which to hang code. In this case, you might construct a tracer consisting of a trivial implementation of the container packing algorithm (maybe something like first-come, first-served) and a simple but working user interface. Once you have all the components in the application plumbed together, you have a framework to show your users and your developers. Over time, you add to this framework with new functionality, completing stubbed routines. But the framework stays intact, and you know the system will continue to behave the way it did when your first tracer code was completed.

The distinction is important enough to warrant repeating. Prototyping generates disposable code. Tracer code is lean but complete, and forms part of the skeleton of the final system. Think of prototyping as the reconnaissance and intelligence gathering that takes place before a single tracer bullet is fired.


Related Sections Include


Topic 13, ​Prototypes and Post-it Notes​

Topic 27, ​Don’t Outrun Your Headlights​

Topic 40, ​Refactoring​

Topic 49, ​Pragmatic Teams​

Topic 50, ​Coconuts Don’t Cut It​

Topic 51, ​Pragmatic Starter Kit​

Topic 52, ​Delight Your Users​


Topic 13

Prototypes and Post-it Notes


Many industries use prototypes to try out specific ideas; prototyping is much cheaper than full-scale production. Car makers, for example, may build many different prototypes of a new car design. Each one is designed to test a specific aspect of the car—the aerodynamics, styling, structural characteristics, and so on. Old school folks might use a clay model for wind tunnel testing, maybe a balsa wood and duct tape model will do for the art department, and so on. The less romantic will do their modeling on a computer screen or in virtual reality, reducing costs even further. In this way, risky or uncertain elements can be tried out without committing to building the real item.

We build software prototypes in the same fashion, and for the same reasons—to analyze and expose risk, and to offer chances for correction at a greatly reduced cost. Like the car makers, we can target a prototype to test one or more specific aspects of a project.

We tend to think of prototypes as code-based, but they don’t always have to be. Like the car makers, we can build prototypes out of different materials. Post-it notes are great for prototyping dynamic things such as workflow and application logic. A user interface can be prototyped as a drawing on a whiteboard, as a nonfunctional mock-up drawn with a paint program, or with an interface builder.

Prototypes are designed to answer just a few questions, so they are much cheaper and faster to develop than applications that go into production. The code can ignore unimportant details—unimportant to you at the moment, but probably very important to the user later on. If you are prototyping a UI, for instance, you can get away with incorrect results or data. On the other hand, if you’re just investigating computational or performance aspects, you can get away with a pretty poor UI, or perhaps even no UI at all.

But if you find yourself in an environment where you cannot give up the details, then you need to ask yourself if you are really building a prototype at all. Perhaps a tracer bullet style of development would be more appropriate in this case (see Topic 12, ​Tracer Bullets​).


Things to Prototype


What sorts of things might you choose to investigate with a prototype? Anything that carries risk. Anything that hasn’t been tried before, or that is absolutely critical to the final system. Anything unproven, experimental, or doubtful. Anything you aren’t comfortable with. You can prototype:

Architecture

New functionality in an existing system

Structure or contents of external data

Third-party tools or components

Performance issues

User interface design


Prototyping is a learning experience. Its value lies not in the code produced, but in the lessons learned. That’s really the point of prototyping.

Tip 21 Prototype to Learn


How to Use Prototypes


When building a prototype, what details can you ignore?

Correctness

You may be able to use dummy data where appropriate.


Completeness

The prototype may function only in a very limited sense, perhaps with only one preselected piece of input data and one menu item.


Robustness

Error checking is likely to be incomplete or missing entirely. If you stray from the predefined path, the prototype may crash and burn in a glorious display of pyrotechnics. That’s okay.


Style

Prototype code shouldn’t have much in the way of comments or documentation (although you may produce reams of documentation as a result of your experience with the prototype).


Prototypes gloss over details, and focus in on specific aspects of the system being considered, so you may want to implement them using a high-level scripting language—higher than the rest of the project (maybe a language such as Python or Ruby), as these languages can get out of your way. You may choose to continue to develop in the language used for the prototype, or you can switch; after all, you’re going to throw the prototype away anyway.

To prototype user interfaces, use a tool that lets you focus on the appearance and/or interactions without worrying about code or markup.

Scripting languages also work well as the “glue’’ to combine low-level pieces into new combinations. Using this approach, you can rapidly assemble existing components into new configurations to see how things work.


Prototyping Architecture


Many prototypes are constructed to model the entire system under consideration. As opposed to tracer bullets, none of the individual modules in the prototype system need to be particularly functional. In fact, you may not even need to code in order to prototype architecture—you can prototype on a whiteboard, with Post-it notes or index cards. What you are looking for is how the system hangs together as a whole, again deferring details. Here are some specific areas you may want to look for in the architectural prototype:

Are the responsibilities of the major areas well defined and appropriate?

Are the collaborations between major components well defined?

Is coupling minimized?

Can you identify potential sources of duplication?

Are interface definitions and constraints acceptable?

Does every module have an access path to the data it needs during execution? Does it have that access when it needs it?


This last item tends to generate the most surprises and the most valuable results from the prototyping experience.


How Not to Use Prototypes


Before you embark on any code-based prototyping, make sure that everyone understands that you are writing disposable code. Prototypes can be deceptively attractive to people who don’t know that they are just prototypes. You must make it very clear that this code is disposable, incomplete, and unable to be completed.

It’s easy to become misled by the apparent completeness of a demonstrated prototype, and project sponsors or management may insist on deploying the prototype (or its progeny) if you don’t set the right expectations. Remind them that you can build a great prototype of a new car out of balsa wood and duct tape, but you wouldn’t try to drive it in rush-hour traffic!

If you feel there is a strong possibility in your environment or culture that the purpose of prototype code may be misinterpreted, you may be better off with the tracer bullet approach. You’ll end up with a solid framework on which to base future development.

Properly used prototypes can save you huge amounts of time, money, and pain by identifying and correcting potential problem spots early in the development cycle—the time when fixing mistakes is both cheap and easy.


Related Sections Include


Topic 12, ​Tracer Bullets​

Topic 14, ​Domain Languages​

Topic 17, ​Shell Games​

Topic 27, ​Don’t Outrun Your Headlights​

Topic 37, ​Listen to Your Lizard Brain​

Topic 45, ​The Requirements Pit​

Topic 52, ​Delight Your Users​


Exercises


Exercise 3 (possible answer)

Marketing would like to sit down and brainstorm a few web page designs with you. They are thinking of clickable image maps to take you to other pages, and so on. But they can’t decide on a model for the image—maybe it’s a car, or a phone, or a house. You have a list of target pages and content; they’d like to see a few prototypes. Oh, by the way, you have 15 minutes. What tools might you use?


Topic 14

Domain Languages


The limits of language are the limits of one’s world.


Ludwig Wittgenstein


Computer languages influence how you think about a problem, and how you think about communicating. Every language comes with a list of features: buzzwords such as static versus dynamic typing, early versus late binding, functional versus OO, inheritance models, mixins, macros—all of which may suggest or obscure certain solutions. Designing a solution with C++ in mind will produce different results than a solution based on Haskell-style thinking, and vice versa. Conversely, and we think more importantly, the language of the problem domain may also suggest a programming solution.

We always try to write code using the vocabulary of the application domain (see ​Maintain a Glossary​). In some cases, Pragmatic Programmers can go to the next level and actually program using the vocabulary, syntax, and semantics—the language—of the domain.

Tip 22 Program Close to the Problem Domain


Some Real-World Domain Languages


Let’s look at a few examples where folks have done just that.


RSpec


RSpec[19] is a testing library for Ruby. It inspired versions for most other modern languages. A test in RSpec is intended to reflect the behavior you expect from your code.

​ describe BowlingScore ​do​

​ it ​"totals 12 if you score 3 four times"​ ​do​

​ score = BowlingScore.new

​ 4.times { score.add_pins(3) }

​ expect(score.total).to eq(12)

​ ​end​

​ ​end​


Cucumber


Cucumber[20] is programming-language neutral way of specifying tests. You run the tests using a version of Cucumber appropriate to the language you’re using. In order to support the natural-language like syntax, you also have to write specific matchers that recognize phrases and extract parameters for the tests.

​ ​Feature​: Scoring

​

​ ​Background​:

​ Given an empty scorecard

​

​ ​Scenario​: bowling a lot of 3s

​ Given I throw a 3

​ And I throw a 3

​ And I throw a 3

​ And I throw a 3

​ Then the score should be 12

Cucumber tests were intended to be read by the customers of the software (although that happens fairly rarely in practice; the following aside considers why that might be).


Why Don't Many Business Users Read Cucumber Features?

One of the reasons that the classic gather requirements, design, code, ship approach doesn’t work is that it is anchored by the concept that we know what the requirements are. But we rarely do. Your business users will have a vague idea of what they want to achieve, but they neither know nor care about the details. That’s part of our value: we intuit intent and convert it to code.

So when you force a business person to sign off on a requirements document, or get them to agree to a set of Cucumber features, you’re doing the equivalent of getting them to check the spelling in an essay written in Sumerian. They’ll make some random changes to save face and sign it off to get you out of their office.

Give them code that runs, however, and they can play with it. That’s where their real needs will surface.


Phoenix Routes


Many web frameworks have a routing facility, mapping incoming HTTP requests onto handler functions in the code. Here’s an example from Phoenix.[21]

​ scope ​"​​/"​, HelloPhoenix ​do​

​ pipe_through ​:browser​ ​# Use the default browser stack​

​

​ get ​"​​/"​, PageController, ​:index​

​ resources ​"​​/users"​, UserController

​ ​end​

This says that requests starting “/” will be run through a series of filters appropriate for browsers. A request to “/” itself will be handled by the index function in the PageController module. The UsersController implements the functions needed to manage a resource accessible via the url /users.


Ansible


Ansible[22] is a tool that configures software, typically on a bunch of remote servers. It does this by reading a specification that you provide, then doing whatever is needed on the servers to make them mirror that spec. The specification can be written in YAML,[23] a language that builds data structures from text descriptions:

​ ---

​ - name: ​install nginx​

​ apt: ​name=nginx state=latest​

​

​ - name: ​ensure nginx is running (and enable it at boot)​

​ service: ​name=nginx state=started enabled=yes​

​

​ - name: ​write the nginx config file​

​ template: ​src=templates/nginx.conf.j2 dest=/etc/nginx/nginx.conf​

​ notify:

​ - ​restart nginx​

This example ensures that the latest version of nginx is installed on my servers, that it is started by default, and that it uses a configuration file that you’ve provided.


Characteristics of Domain Languages


Let’s look at these examples more closely.

RSpec and the Phoenix router are written in their host languages (Ruby and Elixir). They employ some fairly devious code, including metaprogramming and macros, but ultimately they are compiled and run as regular code.

Cucumber tests and Ansible configurations are written in their own languages. A Cucumber test is converted into code to be run or into a datastructure, whereas Ansible specs are always converted into a data structure that is run by Ansible itself.

As a result, RSpec and the router code are embedded into the code you run: they are true extensions to your code’s vocabulary. Cucumber and Ansible are read by code and converted into some form the code can use.

We call RSpec and the router examples of internal domain languages, while Cucumber and Ansible use external languages.


Trade-Offs Between Internal and External Languages


In general, an internal domain language can take advantage of the features of its host language: the domain language you create is more powerful, and that power comes for free. For example, you could use some Ruby code to create a bunch of RSpec tests automatically. In this case we can test scores where there are no spares or strikes:

​ describe BowlingScore ​do​

​ (0..4).each ​do​ |pins|

​ (1..20).each ​do​ |throws|

​ target = pins * throws

​

​ it ​"totals ​​#{​target​}​​ if you score ​​#{​pins​}​​ ​​#{​throws​}​​ times"​ ​do​

​ score = BowlingScore.new

​ throws.times { score.add_pins(pins) }

​ expect(score.total).to eq(target)

​ ​end​

​ ​end​

​ ​end​

​ ​end​

That’s 100 tests you just wrote. Take the rest of the day off.

The downside of internal domain languages is that you’re bound by the syntax and semantics of that language. Although some languages are remarkably flexible in this regards, you’re still forced to compromise between the language you want and the language you can implement.

Ultimately, whatever you come up with must still be valid syntax in your target language. Languages with macros (such as Elixir, Clojure, and Crystal) gives you a little more flexibility, but ultimately syntax is syntax.

External languages have no such restrictions. As long as you can write a parser for the language, you’re good to go. Sometimes you can use someone else’s parser (as Ansible did by using YAML), but then you’re back to making a compromise.

Writing a parser probably means adding new libraries and possibly tools to your application. And writing a good parser is not a trivial job. But, if you’re feeling stout of heart, you could look at parser generators such as bison or ANTLR, and parsing frameworks such as the many PEG parsers out there.

Our suggestion is fairly simple: don’t spend more effort than you save. Writing a domain language adds some cost to your project, and you’ll need to be convinced that there are offsetting savings (potentially in the long term).

In general, use off-the-shelf external languages (such as YAML, JSON, or CSV) if you can. If not, look at internal languages. We’d recommend using external languages only in cases where your language will be written by the users of your application.


An Internal Domain Language on the Cheap


Finally, there’s a cheat for creating internal domain languages if you don’t mind the host language syntax leaking through. Don’t do a bunch of metaprogramming. Instead, just write functions to do the work. In fact, this is pretty much what RSpec does:

​ describe BowlingScore ​do​

​ it ​"totals 12 if you score 3 four times"​ ​do​

​ score = BowlingScore.new

​ 4.times { score.add_pins(3) }

​ expect(score.total).to eq(12)

​ ​end​

​ ​end​

In this code, describe, it, expect, to, and eq are just Ruby methods. There’s a little plumbing behind the scenes in terms of how objects are passed around, but it’s all just code. We’ll explore that a little in the exercises.


Related Sections Include


Topic 8, ​The Essence of Good Design​

Topic 13, ​Prototypes and Post-it Notes​

Topic 32, ​Configuration​


Challenges


Could some of the requirements of your current project be expressed in a domain-specific language? Would it be possible to write a compiler or translator that could generate most of the code required?


If you decide to adopt mini-languages as a way of programming closer to the problem domain, you’re accepting that some effort will be required to implement them. Can you see ways in which the framework you develop for one project can be reused in others?


Exercises


Exercise 4 (possible answer)

We want to implement a mini-language to control a simple turtle-graphics system. The language consists of single-letter commands, some followed by a single number. For example, the following input would draw a rectangle:

​ P 2 # select pen 2

​ D # pen down

​ W 2 # draw west 2cm

​ N 1 # then north 1

​ E 2 # then east 2

​ S 1 # then back south

​ U # pen up

Implement the code that parses this language. It should be designed so that it is simple to add new commands.

Exercise 5 (possible answer)

In the previous exercise we implemented a parser for the drawing language—it was an external domain language. Now implement it again as an internal language. Don’t do anything clever: just write a function for each of the commands. You may have to change the names of the commands to lower case, and maybe to wrap them inside something to provide some context.

Exercise 6 (possible answer)

Design a BNF grammar to parse a time specification. All of the following examples should be accepted:

​ 4pm, 7:38pm, 23:42, 3:16, 3:16am

Exercise 7 (possible answer)

Implement a parser for the BNF grammar in the previous exercise using a PEG parser generator in the language of your choice. The output should be an integer containing the number of minutes past midnight.

Exercise 8 (possible answer)

Implement the time parser using a scripting language and regular expressions.


Topic 15

Estimating


The Library of Congress in Washington, DC, currently has about 75 terabytes of digital information online. Quick! How long will it take to send all that information over a 1Gbps network? How much storage will you need for a million names and addresses? How long does it take to compress 100Mb of text? How many months will it take to deliver your project?

At one level, these are all meaningless questions—they are all missing information. And yet they can all be answered, as long as you are comfortable estimating. And, in the process of producing an estimate, you’ll come to understand more about the world your programs inhabit.

By learning to estimate, and by developing this skill to the point where you have an intuitive feel for the magnitudes of things, you will be able to show an apparent magical ability to determine their feasibility. When someone says “we’ll send the backup over a network connection to S3,” you’ll be able to know intuitively whether this is practical. When you’re coding, you’ll be able to know which subsystems need optimizing and which ones can be left alone.

Tip 23 Estimate to Avoid Surprises

As a bonus, at the end of this section we’ll reveal the single correct answer to give whenever anyone asks you for an estimate.


How Accurate Is Accurate Enough?


To some extent, all answers are estimates. It’s just that some are more accurate than others. So the first question you have to ask yourself when someone asks you for an estimate is the context in which your answer will be taken. Do they need high accuracy, or are they looking for a ballpark figure?

One of the interesting things about estimating is that the units you use make a difference in the interpretation of the result. If you say that something will take about 130 working days, then people will be expecting it to come in pretty close. However, if you say “Oh, about six months,” then they know to look for it any time between five and seven months from now. Both numbers represent the same duration, but “130 days” probably implies a higher degree of accuracy than you feel. We recommend that you scale time estimates as follows:


Duration

Quote estimate in


1–15 days

Days


3–6 weeks

Weeks


8–20 weeks

Months


20+ weeks

Think hard before giving an estimate


So, if after doing all the necessary work, you decide that a project will take 125 working days (25 weeks), you might want to deliver an estimate of “about six months.”

The same concepts apply to estimates of any quantity: choose the units of your answer to reflect the accuracy you intend to convey.


Where Do Estimates Come From?


All estimates are based on models of the problem. But before we get too deeply into the techniques of building models, we have to mention a basic estimating trick that always gives good answers: ask someone who’s already done it. Before you get too committed to model building, cast around for someone who’s been in a similar situation in the past. See how their problem got solved. It’s unlikely you’ll ever find an exact match, but you’d be surprised how many times you can successfully draw on others’ experiences.


Understand What’s Being Asked


The first part of any estimation exercise is building an understanding of what’s being asked. As well as the accuracy issues discussed above, you need to have a grasp of the scope of the domain. Often this is implicit in the question, but you need to make it a habit to think about the scope before starting to guess. Often, the scope you choose will form part of the answer you give: “Assuming there are no traffic accidents and there’s gas in the car, I should be there in 20 minutes.”


Build a Model of the System


This is the fun part of estimating. From your understanding of the question being asked, build a rough-and-ready bare-bones mental model. If you’re estimating response times, your model may involve a server and some kind of arriving traffic. For a project, the model may be the steps that your organization uses during development, along with a very rough picture of how the system might be implemented.

Model building can be both creative and useful in the long term. Often, the process of building the model leads to discoveries of underlying patterns and processes that weren’t apparent on the surface. You may even want to reexamine the original question: “You asked for an estimate to do X. However, it looks like Y, a variant of X, could be done in about half the time, and you lose only one feature.”

Building the model introduces inaccuracies into the estimating process. This is inevitable, and also beneficial. You are trading off model simplicity for accuracy. Doubling the effort on the model may give you only a slight increase in accuracy. Your experience will tell you when to stop refining.


Break the Model into Components


Once you have a model, you can decompose it into components. You’ll need to discover the mathematical rules that describe how these components interact. Sometimes a component contributes a single value that is added into the result. Some components may supply multiplying factors, while others may be more complicated (such as those that simulate the arrival of traffic at a node).

You’ll find that each component will typically have parameters that affect how it contributes to the overall model. At this stage, simply identify each parameter.


Give Each Parameter a Value


Once you have the parameters broken out, you can go through and assign each one a value. You expect to introduce some errors in this step. The trick is to work out which parameters have the most impact on the result, and concentrate on getting them about right. Typically, parameters whose values are added into a result are less significant than those that are multiplied or divided. Doubling a line speed may double the amount of data received in an hour, while adding a 5ms transit delay will have no noticeable effect.

You should have a justifiable way of calculating these critical parameters. For the queuing example, you might want to measure the actual transaction arrival rate of the existing system, or find a similar system to measure. Similarly, you could measure the current time taken to serve a request, or come up with an estimate using the techniques described in this section. In fact, you’ll often find yourself basing an estimate on other subestimates. This is where your largest errors will creep in.


Calculate the Answers


Only in the simplest of cases will an estimate have a single answer. You might be happy to say “I can walk five cross-town blocks in 15 minutes.” However, as the systems get more complex, you’ll want to hedge your answers. Run multiple calculations, varying the values of the critical parameters, until you work out which ones really drive the model. A spreadsheet can be a big help. Then couch your answer in terms of these parameters. “The response time is roughly three quarters of a second if the system has SSDs and 32GB of memory, and one second with 16GB memory.” (Notice how “three quarters of a second” conveys a different feeling of accuracy than 750ms.)

During the calculation phase, you get answers that seem strange. Don’t be too quick to dismiss them. If your arithmetic is correct, your understanding of the problem or your model is probably wrong. This is valuable information.


Keep Track of Your Estimating Prowess


We think it’s a great idea to record your estimates so you can see how close you were. If an overall estimate involved calculating subestimates, keep track of these as well. Often you’ll find your estimates are pretty good—in fact, after a while, you’ll come to expect this.

When an estimate turns out wrong, don’t just shrug and walk away—find out why. Maybe you chose some parameters that didn’t match the reality of the problem. Maybe your model was wrong. Whatever the reason, take some time to uncover what happened. If you do, your next estimate will be better.


Estimating Project Schedules


Normally you’ll be asked to estimate how long something will take. If that “something” is complex, the estimate can be very difficult to produce. In this section, we’ll look at two techniques for reducing that uncertainty.


Painting the Missile


“How long will it take to paint the house?”

“Well, if everything goes right, and this paint has the coverage they claim, it might be as few as 10 hours. But that’s unlikely: I’d guess a more realistic figure is closer to 18 hours. And, of course, if the weather turns bad, that could push it out to 30 or more.”

That’s how people estimate in the real world. Not with a single number (unless you force them to give you one) but with a range of scenarios.

When the U.S. Navy needed to plan the Polaris submarine project, they adopted this style of estimating with a methodology they called the Program Evaluation Review Technique, or PERT.

Every PERT task has an optimistic, a most likely, and a pessimistic estimate. The tasks are arranged into a dependency network, and then you use some simple statistics to identify likely best and worst times for the overall project.

Using a range of values like this is a great way to avoid one of the most common causes of estimation error: padding a number because you’re unsure. Instead, the statistics behind PERT spreads the uncertainty out for you, giving you better estimations of the whole project.

However, we’re not big fans of this. People tend to produce wall-sized charts of all the tasks in a project, and implicitly believe that, just because they used a formula, they have an accurate estimate. The chances are they don’t, because they have never done this before.


Eating the Elephant


We find that often the only way to determine the timetable for a project is by gaining experience on that same project. This needn’t be a paradox if you practice incremental development, repeating the following steps with very thin slices of functionality:

Check requirements

Analyze risk (and prioritize riskiest items earlier)

Design, implement, integrate

Validate with the users


Initially, you may have only a vague idea of how many iterations will be required, or how long they may be. Some methods require you to nail this down as part of the initial plan; however, for all but the most trivial of projects this is a mistake. Unless you are doing an application similar to a previous one, with the same team and the same technology, you’d just be guessing.

So you complete the coding and testing of the initial functionality and mark this as the end of the first iteration. Based on that experience, you can refine your initial guess on the number of iterations and what can be included in each. The refinement gets better and better each time, and confidence in the schedule grows along with it. This kind of estimating is often done during the team’s review at the end of each iterative cycle.

That’s also how the old joke says to eat an elephant: one bite at a time.

Tip 24 Iterate the Schedule with the Code

This may not be popular with management, who typically want a single, hard-and-fast number before the project even starts. You’ll have to help them understand that the team, their productivity, and the environment will determine the schedule. By formalizing this, and refining the schedule as part of each iteration, you’ll be giving them the most accurate scheduling estimates you can.


What to Say When Asked for an Estimate


You say “I’ll get back to you.”

You almost always get better results if you slow the process down and spend some time going through the steps we describe in this section. Estimates given at the coffee machine will (like the coffee) come back to haunt you.


Related Sections Include


Topic 7, ​Communicate!​

Topic 39, ​Algorithm Speed​


Challenges


Start keeping a log of your estimates. For each, track how accurate you turned out to be. If your error was greater than 50%, try to find out where your estimate went wrong.


Exercises


Exercise 9 (possible answer)

You are asked “Which has a higher bandwidth: a 1Gbps net connection or a person walking between two computers with a full 1TB of storage device in their pocket?’’ What constraints will you put on your answer to ensure that the scope of your response is correct? (For example, you might say that the time taken to access the storage device is ignored.)

Exercise 10 (possible answer)

So, which has the higher bandwidth?


Footnotes


[13]

To paraphrase the old Arlen/Mercer song…


[14]

Or, perhaps, to keep your sanity, every 10th time…


[15]

https://github.com/OAI/OpenAPI-Specification


[16]

In reality, this is naive. Unless you are remarkably lucky, most real-world requirements changes will affect multiple functions in the system. However, if you analyze the change in terms of functions, each functional change should still ideally affect just one module.


[17]

In fact, this book is written in Markdown, and typeset directly from the Markdown source.


[18]

Take a nonlinear, or chaotic, system and apply a small change to one of its inputs. You may get a large and often unpredictable result. The clichéd butterfly flapping its wings in Tokyo could be the start of a chain of events that ends up generating a tornado in Texas. Does this sound like any projects you know?


[19]

https://rspec.info


[20]

https://cucumber.io/


[21]

https://phoenixframework.org/


[22]

https://www.ansible.com/


[23]

https://yaml.org/





Chapter 3


The Basic Tools


Every maker starts their journey with a basic set of good-quality tools. A woodworker might need rules, gauges, a couple of saws, some good planes, fine chisels, drills and braces, mallets, and clamps. These tools will be lovingly chosen, will be built to last, will perform specific jobs with little overlap with other tools, and, perhaps most importantly, will feel right in the budding woodworker’s hands.

Then begins a process of learning and adaptation. Each tool will have its own personality and quirks, and will need its own special handling. Each must be sharpened in a unique way, or held just so. Over time, each will wear according to use, until the grip looks like a mold of the woodworker’s hands and the cutting surface aligns perfectly with the angle at which the tool is held. At this point, the tools become conduits from the maker’s brain to the finished product—they have become extensions of their hands. Over time, the woodworker will add new tools, such as biscuit cutters, laser-guided miter saws, dovetail jigs—all wonderful pieces of technology. But you can bet that they’ll be happiest with one of those original tools in hand, feeling the plane sing as it slides through the wood.

Tools amplify your talent. The better your tools, and the better you know how to use them, the more productive you can be. Start with a basic set of generally applicable tools. As you gain experience, and as you come across special requirements, you’ll add to this basic set. Like the maker, expect to add to your toolbox regularly. Always be on the lookout for better ways of doing things. If you come across a situation where you feel your current tools can’t cut it, make a note to look for something different or more powerful that would have helped. Let need drive your acquisitions.

Many new programmers make the mistake of adopting a single power tool, such as a particular integrated development environment (IDE), and never leave its cozy interface. This really is a mistake. You need to be comfortable beyond the limits imposed by an IDE. The only way to do this is to keep the basic tool set sharp and ready to use.

In this chapter we’ll talk about investing in your own basic toolbox. As with any good discussion on tools, we’ll start (in Topic 16, ​The Power of Plain Text​) by looking at your raw materials, the stuff you’ll be shaping. From there we’ll move to the workbench, or in our case the computer. How can you use your computer to get the most out of the tools you use? We’ll discuss this in Topic 17, ​Shell Games​. Now that we have material and a bench to work on, we’ll turn to the tool you’ll probably use more than any other, your editor. In Topic 18, ​Power Editing​, we’ll suggest ways of making you more efficient.

To ensure that we never lose any of our precious work, we should always use a Topic 19, ​Version Control​ system—even for personal things such as recipes or notes. And, since Murphy was really an optimist after all, you can’t be a great programmer until you become highly skilled at Topic 20, ​Debugging​.

You’ll need some glue to bind much of the magic together. We discuss some possibilities in Topic 21, ​Text Manipulation​.

Finally, the palest ink is still better than the best memory. Keep track of your thoughts and your history, as we describe in Topic 22, ​Engineering Daybooks​.

Spend time learning to use these tools, and at some point you’ll be surprised to discover your fingers moving over the keyboard, manipulating text without conscious thought. The tools will have become extensions of your hands.


Topic 16

The Power of Plain Text


As Pragmatic Programmers, our base material isn’t wood or iron, it’s knowledge. We gather requirements as knowledge, and then express that knowledge in our designs, implementations, tests, and documents. And we believe that the best format for storing knowledge persistently is plain text. With plain text, we give ourselves the ability to manipulate knowledge, both manually and programmatically, using virtually every tool at our disposal.

The problem with most binary formats is that the context necessary to understand the data is separate from the data itself. You are artificially divorcing the data from its meaning. The data may as well be encrypted; it is absolutely meaningless without the application logic to parse it. With plain text, however, you can achieve a self-describing data stream that is independent of the application that created it.


What Is Plain Text?


Plain text is made up of printable characters in a form that conveys information. It can be as simple as a shopping list:

​ * milk

​ * lettuce

​ * coffee

or as complex as the source of this book (yes, it’s in plain text, much to the chagrin of the publisher, who wanted us to use a word processor).

The information part is important. The following is not useful plain text:

​ hlj;uijn bfjxrrctvh jkni'pio6p7gu;vh bjxrdi5rgvhj

Neither is this:

​ Field19=467abe

The reader has no idea what the significance of 467abe may be. We like our plain text to be understandable to humans.

Tip 25 Keep Knowledge in Plain Text


The Power of Text


Plain text doesn’t mean that the text is unstructured; HTML, JSON, YAML, and so on are all plain text. So are the majority of the fundamental protocols on the net, such as HTTP, SMTP, IMAP, and so on. And that’s for some good reasons:

Insurance against obsolescence

Leverage existing tools

Easier testing


Insurance Against Obsolescence


Human-readable forms of data, and self-describing data, will outlive all other forms of data and the applications that created them. Period. As long as the data survives, you will have a chance to be able to use it—potentially long after the original application that wrote it is defunct.

You can parse such a file with only partial knowledge of its format; with most binary files, you must know all the details of the entire format in order to parse it successfully.

Consider a data file from some legacy system that you are given.[24] You know little about the original application; all that’s important to you is that it maintained a list of clients’ Social Security numbers, which you need to find and extract. Among the data, you see

​ <FIELD10>123-45-6789</FIELD10>

​ ...

​ <FIELD10>567-89-0123</FIELD10>

​ ...

​ <FIELD10>901-23-4567</FIELD10>

Recognizing the format of a Social Security number, you can quickly write a small program to extract that data—even if you have no information on anything else in the file.

But imagine if the file had been formatted this way instead:

​ AC27123456789B11P

​ ...

​ XY43567890123QTYL

​ ...

​ 6T2190123456788AM

You may not have recognized the significance of the numbers quite as easily. This is the difference between human readable and human understandable.

While we’re at it, FIELD10 doesn’t help much either. Something like

​ <SOCIAL-SECURITY-NO>123-45-6789</SOCIAL-SECURITY-NO>

makes the exercise a no-brainer—and ensures that the data will outlive any project that created it.


Leverage


Virtually every tool in the computing universe, from version control systems to editors to command-line tools, can operate on plain text.


The Unix Philosophy

Unix is famous for being designed around the philosophy of small, sharp tools, each intended to do one thing well. This philosophy is enabled by using a common underlying format—the line-oriented, plain-text file. Databases used for system administration (users and passwords, networking configuration, and so on) are all kept as plain-text files. (Some systems also maintain a binary form of certain databases as a performance optimization. The plain-text version is kept as an interface to the binary version.)

When a system crashes, you may be faced with only a minimal environment to restore it (you may not be able to access graphics drivers, for instance). Situations such as this can really make you appreciate the simplicity of plain text.

Plain text is also easier to search. If you can’t remember which configuration file manages your system backups, a quick grep -r backup /etc should tell you.


For instance, suppose you have a production deployment of a large application with a complex site-specific configuration file. If this file is in plain text, you could place it under a version control system (see Topic 19, ​Version Control​), so that you automatically keep a history of all changes. File comparison tools such as diff and fc allow you to see at a glance what changes have been made, while sum allows you to generate a checksum to monitor the file for accidental (or malicious) modification.


Easier Testing


If you use plain text to create synthetic data to drive system tests, then it is a simple matter to add, update, or modify the test data without having to create any special tools to do so. Similarly, plain-text output from regression tests can be trivially analyzed with shell commands or a simple script.


Lowest Common Denominator


Even in the future of blockchain-based intelligent agents that travel the wild and dangerous internet autonomously, negotiating data interchange among themselves, the ubiquitous text file will still be there. In fact, in heterogeneous environments the advantages of plain text can outweigh all of the drawbacks. You need to ensure that all parties can communicate using a common standard. Plain text is that standard.


Related Sections Include


Topic 17, ​Shell Games​

Topic 21, ​Text Manipulation​

Topic 32, ​Configuration​


Challenges


Design a small address book database (name, phone number, and so on) using a straightforward binary representation in your language of choice. Do this before reading the rest of this challenge.

Translate that format into a plain-text format using XML or JSON.


For each version, add a new, variable-length field called directions in which you might enter directions to each person’s house.


What issues come up regarding versioning and extensibility? Which form was easier to modify? What about converting existing data?


Topic 17

Shell Games


Every woodworker needs a good, solid, reliable workbench, somewhere to hold work pieces at a convenient height while they’re being shaped. The workbench becomes the center of the woodshop, the maker returning to it time and time again as a piece takes shape.

For a programmer manipulating files of text, that workbench is the command shell. From the shell prompt, you can invoke your full repertoire of tools, using pipes to combine them in ways never dreamt of by their original developers. From the shell, you can launch applications, debuggers, browsers, editors, and utilities. You can search for files, query the status of the system, and filter output. And by programming the shell, you can build complex macro commands for activities you perform often.

For programmers raised on GUI interfaces and integrated development environments (IDEs), this might seem an extreme position. After all, can’t you do everything equally well by pointing and clicking?

The simple answer is “no.’’ GUI interfaces are wonderful, and they can be faster and more convenient for some simple operations. Moving files, reading and writing email, and building and deploying your project are all things that you might want to do in a graphical environment. But if you do all your work using GUIs, you are missing out on the full capabilities of your environment. You won’t be able to automate common tasks, or use the full power of the tools available to you. And you won’t be able to combine your tools to create customized macro tools. A benefit of GUIs is WYSIWYG—what you see is what you get. The disadvantage is WYSIAYG—what you see is all you get.

GUI environments are normally limited to the capabilities that their designers intended. If you need to go beyond the model the designer provided, you are usually out of luck—and more often than not, you do need to go beyond the model. Pragmatic Programmers don’t just cut code, or develop object models, or write documentation, or automate the build process—we do all of these things. The scope of any one tool is usually limited to the tasks that the tool is expected to perform. For instance, suppose you need to integrate a code preprocessor (to implement design-by-contract, or multi-processing pragmas, or some such) into your IDE. Unless the designer of the IDE explicitly provided hooks for this capability, you can’t do it.

Tip 26 Use the Power of Command Shells

Gain familiarity with the shell, and you’ll find your productivity soaring. Need to create a list of all the unique package names explicitly imported by your Java code? The following stores it in a file called “list’’:

sh/packages.sh

​ grep ​'^import '​ ​*​.java |

​ sed -e​'s/.*import *//'​ -e​'s/;.*$//'​ |

​ sort -u >list

If you haven’t spent much time exploring the capabilities of the command shell on the systems you use, this might appear daunting. However, invest some energy in becoming familiar with your shell and things will soon start falling into place. Play around with your command shell, and you’ll be surprised at how much more productive it makes you.


A Shell of Your Own


In the same way that a woodworker will customize their workspace, a developer should customize their shell. This typically also involves changing the configuration of the terminal program you use.

Common changes include:

Setting color themes. Many, many hours can be spent trying out every single theme that’s available online for your particular shell.


Configuring a prompt. The prompt that tells you the shell is ready for you to type a command can be configured to display just about any information you might want (and a bunch of stuff you’d never want). Personal preferences are everything here: we tend to like simple prompts, with a shortened current directory name and version control status along with the time.


Aliases and shell functions. Simplify your workflow by turning commands you use a lot into simple aliases. Maybe you regularly update your Linux box, but can never remember whether you update and upgrade, or upgrade and update. Create an alias:

​ alias apt-up=​'sudo apt-get update && sudo apt-get upgrade'​

Maybe you’ve accidentally deleted files with the rm command just one time too often. Write an alias so that it will always prompt in future:

​ alias rm =​'rm -iv'​


Command completion. Most shells will complete the names of commands and files: type the first few characters, hit tab, and it’ll fill in what it can. But you can take this a lot further, configuring the shell to recognize the command you’re entering and offer context-specific completions. Some even customize the completion depending on the current directory.


You’ll spend a lot of time living in one of these shells. Be like a hermit crab and make it your own home.


Related Sections Include


Topic 13, ​Prototypes and Post-it Notes​

Topic 16, ​The Power of Plain Text​

Topic 21, ​Text Manipulation​

Topic 30, ​Transforming Programming​

Topic 51, ​Pragmatic Starter Kit​


Challenges


Are there things that you’re currently doing manually in a GUI? Do you ever pass instructions to colleagues that involve a number of individual “click this button,” “select this item” steps? Could these be automated?


Whenever you move to a new environment, make a point of finding out what shells are available. See if you can bring your current shell with you.


Investigate alternatives to your current shell. If you come across a problem your shell can’t address, see if an alternative shell would cope better.


Topic 18

Power Editing


We’ve talked before about tools being an extension of your hand. Well, this applies to editors more than to any other software tool. You need to be able to manipulate text as effortlessly as possible, because text is the basic raw material of programming.

In the first edition of this book we recommended using a single editor for everything: code, documentation, memos, system administration, and so on. We’ve softened that position a little. We’re happy for you to use as many editors as you want. We’d just like you to be working toward fluency in each.

Tip 27 Achieve Editor Fluency

Why is this a big deal? Are we saying you’ll save lots of time? Actually yes: over the course of a year, you might actually gain an additional week if you make your editing just 4% more efficient and you edit for 20 hours a week.

But that’s not the real benefit. No, the major gain is that by becoming fluent, you no longer have to think about the mechanics of editing. The distance between thinking something and having it appear in an editor buffer drop way down. Your thoughts will flow, and your programming will benefit. (If you’ve ever taught someone to drive, then you’ll understand the difference between someone who has to think about every action they take and a more experienced driver who controls the car instinctively.)


What Does “Fluent” Mean?


What counts as being fluent? Here’s the challenge list:

When editing text, move and make selections by character, word, line, and paragraph.


When editing code, move by various syntactic units (matching delimiters, functions, modules, …).


Reindent code following changes.


Comment and uncomment blocks of code with a single command.


Undo and redo changes.


Split the editor window into multiple panels, and navigate between them.


Navigate to a particular line number.


Sort selected lines.


Search for both strings and regular expressions, and repeat previous searches.


Temporarily create multiple cursors based on a selection or on a pattern match, and edit the text at each in parallel.


Display compilation errors in the current project.


Run the current project’s tests.


Can you do all this without using a mouse/trackpad?

You might say that your current editor can’t do some of these things. Maybe it’s time to switch?


Moving Toward Fluency


We doubt there are more than a handful of people who know all the commands in any particular powerful editor. We don’t expect you to, either. Instead, we suggest a more pragmatic approach: learn the commands that make your life easier.

The recipe for this is fairly simple.

First, look at yourself while you’re editing. Every time you find yourself doing something repetitive, get into the habit of thinking “there must be a better way.” Then find it.

Once you’ve discovered a new, useful feature, you now need to get it installed into your muscle memory, so you can use it without thinking. The only way we know to do that is through repetition. Consciously look for opportunities to use your new superpower, ideally many times a day. After a week or so, you’ll find you use it without thinking.


Growing Your Editor


Most of the powerful code editors are built around a basic core that is then augmented through extensions. Many are supplied with the editor, and others can be added later.

When you bump into some apparent limitation of the editor you’re using, search around for an extension that will do the job. The chances are that you are not alone in needing that capability, and if you’re lucky someone else will have published their solution.

Take this a step further. Dig into your editor’s extension language. Work out how to use it to automate some of the repetitive things you do. Often you’ll just need a line or two of code.

Sometimes you might take it further still, and you’ll find yourself writing a full-blown extension. If so, publish it: if you had a need for it, other people will, too.


Related Sections Include


Topic 7, ​Communicate!​


Challenges


No more autorepeat.

Everyone does it: you need to delete the last word you typed, so you press down on backspace and wait for autorepeat to kick in. In fact, we bet that your brain has done this so much that you can judge pretty much exactly when to release the key.

So turn off autorepeat, and instead learn the key sequences to move, select, and delete by characters, words, lines, and blocks.


This one is going to hurt.

Lose the mouse/trackpad. For one whole week, edit using just the keyboard. You’ll discover a bunch of stuff that you can’t do without pointing and clicking, so now’s the time to learn. Keep notes (we recommend going old-school and using pencil and paper) of the key sequences you learn.

You’ll take a productivity hit for a few days. But, as you learn to do stuff without moving your hands away from the home position, you’ll find that your editing becomes faster and more fluent than it ever was in the past.


Look for integrations. While writing this chapter, Dave wondered if he could preview the final layout (a PDF file) in an editor buffer. One download later, the layout is sitting alongside the original text, all in the editor. Keep a list of things you’d like to bring into your editor, then look for them.


Somewhat more ambitiously, if you can’t find a plugin or extension that does what you want, write one. Andy is fond of making custom, local file-based Wiki plugins for his favorite editors. If you can’t find it, build it!


Topic 19

Version Control


Progress, far from consisting in change, depends on retentiveness. Those who cannot remember the past are condemned to repeat it.


George Santayana, Life of Reason


One of the important things we look for in a user interface is the undo key—a single button that forgives us our mistakes. It’s even better if the environment supports multiple levels of undo and redo, so you can go back and recover from something that happened a couple of minutes ago.

But what if the mistake happened last week, and you’ve turned your computer on and off ten times since then? Well, that’s one of the many benefits of using a version control system (VCS): it’s a giant undo key—a project-wide time machine that can return you to those halcyon days of last week, when the code actually compiled and ran.

For many folks, that’s the limit of their VCS usage. Those folks are missing out on a whole bigger world of collaboration, deployment pipelines, issue tracking, and general team interaction.

So let’s take a look at VCS, first as a repository of changes, and then as a central meeting place for your team and their code.


Shared Directories Are NOT Version Control

We still come across the occasional team who share their project source files across a network: either internally or using some kind of cloud storage.

This is not viable.

Teams that do this are constantly messing up each other’s work, losing changes, breaking builds, and getting into fist fights in the car park. It’s like writing concurrent code with shared data and no synchronization mechanism. Use version control.

But there’s more! Some folks do use version control, and keep their main repository on a network or cloud drive. They reason that this is the best of both worlds: their files are accessible anywhere and (in the case of cloud storage) it’s backed up off-site.

Turns out that this is even worse, and you risk losing everything. The version control software uses a set of interacting files and directories. If two instances simultaneously make changes, the overall state can become corrupted, and there’s no telling how much damage will be done. And no one likes seeing developers cry.


It Starts at the Source


Version control systems keep track of every change you make in your source code and documentation. With a properly configured source code control system, you can always go back to a previous version of your software.

But a version control system does far more than undo mistakes. A good VCS will let you track changes, answering questions such as: Who made changes in this line of code? What’s the difference between the current version and last week’s? How many lines of code did we change in this release? Which files get changed most often? This kind of information is invaluable for bug-tracking, audit, performance, and quality purposes.

A VCS will also let you identify releases of your software. Once identified, you will always be able to go back and regenerate the release, independent of changes that may have occurred later.

Version control systems may keep the files they maintain in a central repository—a great candidate for archiving.

Finally, version control systems allow two or more users to be working concurrently on the same set of files, even making concurrent changes in the same file. The system then manages the merging of these changes when the files are sent back to the repository. Although seemingly risky, such systems work well in practice on projects of all sizes.

Tip 28 Always Use Version Control

Always. Even if you are a single-person team on a one-week project. Even if it’s a “throw-away’’ prototype. Even if the stuff you’re working on isn’t source code. Make sure that everything is under version control: documentation, phone number lists, memos to vendors, makefiles, build and release procedures, that little shell script that tidies up log files—everything. We routinely use version control on just about everything we type (including the text of this book). Even if we’re not working on a project, our day-to-day work is secured in a repository.


Branching Out


Version control systems don’t just keep a single history of your project. One of their most powerful and useful features is the way they let you isolate islands of development into things called branches. You can create a branch at any point in your project’s history, and any work you do in that branch will be isolated from all other branches. At some time in the future you can merge the branch you’re working on back into another branch, so the target branch now contains the changes you made in your branch. Multiple people can even be working on a branch: in a way, branches are like little clone projects.

One benefit of branches is the isolation they give you. If you develop feature A in one branch, and a teammate works on feature B in another, you’re not going to interfere with each other.

A second benefit, which may be surprising, is that branches are often at the heart of a team’s project workflow.

And this is where things get a little confusing. Version control branches and test organization have something in common: they both have thousands of people out there telling you how you should do it. And that advice is largely meaningless, because what they’re really saying is “this is what worked for me.”

So use version control in your project, and if you bump into workflow issues, search for possible solutions. And remember to review and adjust what you’re doing as you gain experience.


A Thought Experiment

Spill an entire cup of tea (English breakfast, with a little milk) onto your laptop keyboard. Take the machine to the smart-person bar, and have them tut and frown. Buy a new computer. Take it home.

How long would it take to get that machine back to the same state it was in (with all the SSH keys, editor configuration, shell setup, installed applications, and so on) at the point where you first lifted that fateful cup? This was an issue one of us faced recently.

Just about everything that defined the configuration and usage of the original machine was stored in version control, including:

All the user preferences and dotfiles

The editor configuration

The list of software installed using Homebrew

The Ansible script used to configure apps

All current projects


The machine was restored by the end of the afternoon.


Version Control as a Project Hub


Although version control is incredibly useful on personal projects, it really comes into its own when working with a team. And much of this value comes from how you host your repository.

Now, many version control systems don’t need any hosting. They are completely decentralized, with each developer cooperating on a peer-to-peer basis. But even with these systems, it’s worth looking into having a central repository, because once you do, you can take advantage of a ton of integrations to make the project flow easier.

Many of the repository systems are open source, so you can install and run them in your company. But that’s not really your line of business, so we’d recommend most people host with a third party. Look for features such as:

Good security and access control

Intuitive UI

The ability to do everything from the command line, too (because you may need to automate it)

Automated builds and tests

Good support for branch merging (sometimes called pull requests)

Issue management (ideally integrated into commits and merges, so you can keep metrics)

Good reporting (a Kanban board-like display of pending issues and tasks can be very useful)

Good team communications: emails or other notifications on changes, a wiki, and so on


Many teams have their VCS configured so that a push to a particular branch will automatically build the system, run the tests, and if successful deploy the new code into production.

Sound scary? Not when you realize you’re using version control. You can always roll it back.


Related Sections Include


Topic 11, ​Reversibility​

Topic 49, ​Pragmatic Teams​

Topic 51, ​Pragmatic Starter Kit​


Challenges


Knowing you can roll back to any previous state using the VCS is one thing, but can you actually do it? Do you know the commands to do it properly? Learn them now, not when disaster strikes and you’re under pressure.


Spend some time thinking about recovering your own laptop environment in case of a disaster. What would you need to recover? Many of the things you need are just text files. If they’re not in a VCS (hosted off your laptop), find a way to add them. Then think about the other stuff: installed applications, system configuration, and so on. How can you express all that stuff in text files so it, too, can be saved?

An interesting experiment, once you’ve made some progress, is to find an old computer you no longer use and see if your new system can be used to set it up.


Consciously explore the features of your current VCS and hosting provider that you’re not using. If your team isn’t using feature branches, experiment with introducing them. The same with pull/merge requests. Continuous integration. Build pipelines. Even continuous deployment. Look into the team communication tools, too: wikis, Kanban boards, and the like.

You don’t have to use any of it. But you do need to know what it does so you can make that decision.


Use version control for nonproject things, too.


Topic 20

Debugging


It is a painful thing

To look at your own trouble and know

That you yourself and no one else has made it


Sophocles, Ajax


The word bug has been used to describe an “object of terror’’ ever since the fourteenth century. Rear Admiral Dr. Grace Hopper, the inventor of COBOL, is credited with observing the first computer bug—literally, a moth caught in a relay in an early computer system. When asked to explain why the machine wasn’t behaving as intended, a technician reported that there was “a bug in the system,” and dutifully taped it—wings and all—into the log book.

Regrettably, we still have bugs in the system, albeit not the flying kind. But the fourteenth century meaning—a bogeyman—is perhaps even more applicable now than it was then. Software defects manifest themselves in a variety of ways, from misunderstood requirements to coding errors. Unfortunately, modern computer systems are still limited to doing what you tell them to do, not necessarily what you want them to do.

No one writes perfect software, so it’s a given that debugging will take up a major portion of your day. Let’s look at some of the issues involved in debugging and some general strategies for finding elusive bugs.


Psychology of Debugging


Debugging is a sensitive, emotional subject for many developers. Instead of attacking it as a puzzle to be solved, you may encounter denial, finger pointing, lame excuses, or just plain apathy.

Embrace the fact that debugging is just problem solving, and attack it as such.

Having found someone else’s bug, you can spend time and energy laying blame on the filthy culprit who created it. In some workplaces this is part of the culture, and may be cathartic. However, in the technical arena, you want to concentrate on fixing the problem, not the blame.

Tip 29 Fix the Problem, Not the Blame

It doesn’t really matter whether the bug is your fault or someone else’s. It is still your problem.


A Debugging Mindset


Before you start debugging, it’s important to adopt the right mindset. You need to turn off many of the defenses you use each day to protect your ego, tune out any project pressures you may be under, and get yourself comfortable. Above all, remember the first rule of debugging:

Tip 30 Don’t Panic

It’s easy to get into a panic, especially if you are facing a deadline, or have a nervous boss or client breathing down your neck while you are trying to find the cause of the bug. But it is very important to step back a pace, and actually think about what could be causing the symptoms that you believe indicate a bug.

If your first reaction on witnessing a bug or seeing a bug report is “that’s impossible,” you are plainly wrong. Don’t waste a single neuron on the train of thought that begins “but that can’t happen” because quite clearly it can, and has.

Beware of myopia when debugging. Resist the urge to fix just the symptoms you see: it is more likely that the actual fault may be several steps removed from what you are observing, and may involve a number of other related things. Always try to discover the root cause of a problem, not just this particular appearance of it.


Where to Start


Before you start to look at the bug, make sure that you are working on code that built cleanly—without warnings. We routinely set compiler warning levels as high as possible. It doesn’t make sense to waste time trying to find a problem that the computer could find for you! We need to concentrate on the harder problems at hand.

When trying to solve any problem, you need to gather all the relevant data. Unfortunately, bug reporting isn’t an exact science. It’s easy to be misled by coincidences, and you can’t afford to waste time debugging coincidences. You first need to be accurate in your observations.

Accuracy in bug reports is further diminished when they come through a third party—you may actually need to watch the user who reported the bug in action to get a sufficient level of detail.

Andy once worked on a large graphics application. Nearing release, the testers reported that the application crashed every time they painted a stroke with a particular brush. The programmer responsible argued that there was nothing wrong with it; he had tried painting with it, and it worked just fine. This dialog went back and forth for several days, with tempers rapidly rising.

Finally, we got them together in the same room. The tester selected the brush tool and painted a stroke from the upper right corner to the lower left corner. The application exploded. “Oh,” said the programmer, in a small voice, who then sheepishly admitted that he had made test strokes only from the lower left to the upper right, which did not expose the bug.

There are two points to this story:

You may need to interview the user who reported the bug in order to gather more data than you were initially given.


Artificial tests (such as the programmer’s single brush stroke from bottom to top) don’t exercise enough of an application. You must brutally test both boundary conditions and realistic end-user usage patterns. You need to do this systematically (see ​Ruthless and Continuous Testing​).


Debugging Strategies


Once you think you know what is going on, it’s time to find out what the program thinks is going on.


Reproducing Bugs


No, our bugs aren’t really multiplying (although some of them are probably old enough to do it legally). We’re talking about a different kind of reproduction.

The best way to start fixing a bug is to make it reproducible. After all, if you can’t reproduce it, how will you know if it is ever fixed?

But we want more than a bug that can be reproduced by following some long series of steps; we want a bug that can be reproduced with a single command. It’s a lot harder to fix a bug if you have to go through 15 steps to get to the point where the bug shows up.

So here’s the most important rule of debugging:

Tip 31 Failing Test Before Fixing Code

Sometimes by forcing yourself to isolate the circumstances that display the bug, you’ll even gain an insight on how to fix it. The act of writing the test informs the solution.


Coder in a Strange Land


All this talk about isolating the bug is fine, when faced with 50,000 lines of code and a ticking clock, what’s a poor coder to do?

First, look at the problem. Is it a crash? It’s always surprising when we teach courses that involve programming how many developers see an exception pop up in red and immediately tab across to the code.

Tip 32 Read the Damn Error Message

’nuf said.


Bad Results


What if it’s not a crash? What if it’s just a bad result?

Get in there with a debugger and use your failing test to trigger the problem.

Before anything else, make sure that you’re also seeing the incorrect value in the debugger. We’ve both wasted hours trying to track down a bug only to discover that this particular run of the code worked fine.

Sometimes the problem is obvious: interest_rate is 4.5 and should be 0.045. More often you have to look deeper to find out why the value is wrong in the first place. Make sure you know how to move up and down the call stack and examine the local stack environment.

We find it often helps to keep pen and paper nearby so we can jot down notes. In particular we often come across a clue and chase it down, only to find it didn’t pan out. If we didn’t jot down where we were when we started the chase, we could lose a lot of time getting back there.

Sometimes you’re looking at a stack trace that seems to scroll on forever. In this case, there’s often a quicker way to find the problem than examining each and every stack frame: use a binary chop. But before we discuss that, let’s look at two other common bug scenarios.


Sensitivity to Input Values


You’ve been there. Your program works fine with all the test data, and survives its first week in production with honor. Then it suddenly crashes when fed a particular dataset.

You can try looking at the place it crashes and work backwards. But sometimes it’s easier to start with the data. Get a copy of the dataset and feed it through a locally running copy of the app, making sure it still crashes. Then binary chop the data until you isolate exactly which input values are leading to the crash.


Regressions Across Releases


You’re on a good team, and you release your software into production. At some point a bug pops up in code that worked OK a week ago. Wouldn’t it be nice if you could identify the specific change that introduced it? Guess what? Binary chop time.


The Binary Chop


Every CS undergraduate has been forced to code a binary chop (sometimes called a binary search). The idea is simple. You’re looking for a particular value in a sorted array. You could just look at each value in turn, but you’d end up looking at roughly half the entries on average until you either found the value you wanted, or you found a value greater than it, which would mean the value’s not in the array.

But it’s faster to use a divide and conquer approach. Choose a value in the middle of the array. If it’s the one you’re looking for, stop. Otherwise you can chop the array in two. If the value you find is greater than the target then you know it must be in the first half of the array, otherwise it’s in the second half. Repeat the procedure in the appropriate subarray, and in no time you’ll have a result. (As we’ll see when we talk about ​Big-O Notation​, a linear search is , and a binary chop is ).

So, the binary chop is way, way faster on any decent sized problem. Let’s see how to apply it to debugging.

When you’re facing a massive stacktrace and you’re trying to find out exactly which function mangled the value in error, you do a chop by choosing a stack frame somewhere in the middle and seeing if the error is manifest there. If it is, then you know to focus on the frames before, otherwise the problem is in the frames after. Chop again. Even if you have 64 frames in the stacktrace, this approach will give you an answer after at most six attempts.

If you find bugs that appear on certain datasets, you might be able to do the same thing. Split the dataset into two, and see if the problem occurs if you feed one or the other through the app. Keep dividing the data until you get a minimum set of values that exhibit the problem.

If your team has introduced a bug during a set of releases, you can use the same type of technique. Create a test that causes the current release to fail. Then choose a half-way release between now and the last known working version. Run the test again, and decide how to narrow your search. Being able to do this is just one of the many benefits of having good version control in your projects. Indeed, many version control systems will take this further and will automate the process, picking releases for you depending on the result of the test.


Logging and/or Tracing


Debuggers generally focus on the state of the program now. Sometimes you need more—you need to watch the state of a program or a data structure over time. Seeing a stack trace can only tell you how you got here directly. It typically can’t tell you what you were doing prior to this call chain, especially in event-based systems.[25]

Tracing statements are those little diagnostic messages you print to the screen or to a file that say things such as “got here” and “value of x = 2.” It’s a primitive technique compared with IDE-style debuggers, but it is peculiarly effective at diagnosing several classes of errors that debuggers can’t. Tracing is invaluable in any system where time itself is a factor: concurrent processes, real-time systems, and event-based applications.

You can use tracing statements to drill down into the code. That is, you can add tracing statements as you descend the call tree.

Trace messages should be in a regular, consistent format as you may want to parse them automatically. For instance, if you needed to track down a resource leak (such as unbalanced file opens/closes), you could trace each open and each close in a log file. By processing the log file with text processing tools or shell commands, you can easily identify where the offending open was occurring.


Rubber Ducking


A very simple but particularly useful technique for finding the cause of a problem is simply to explain it to someone else. The other person should look over your shoulder at the screen, and nod his or her head constantly (like a rubber duck bobbing up and down in a bathtub). They do not need to say a word; the simple act of explaining, step by step, what the code is supposed to do often causes the problem to leap off the screen and announce itself.[26]

It sounds simple, but in explaining the problem to another person you must explicitly state things that you may take for granted when going through the code yourself. By having to verbalize some of these assumptions, you may suddenly gain new insight into the problem. And if you don’t have a person, a rubber duck, or teddy bear, or potted plant will do.[27]


Process of Elimination


In most projects, the code you are debugging may be a mixture of application code written by you and others on your project team, third-party products (database, connectivity, web framework, specialized communications or algorithms, and so on) and the platform environment (operating system, system libraries, and compilers).

It is possible that a bug exists in the OS, the compiler, or a third-party product—but this should not be your first thought. It is much more likely that the bug exists in the application code under development. It is generally more profitable to assume that the application code is incorrectly calling into a library than to assume that the library itself is broken. Even if the problem does lie with a third party, you’ll still have to eliminate your code before submitting the bug report.

We worked on a project where a senior engineer was convinced that the select system call was broken on a Unix system. No amount of persuasion or logic could change his mind (the fact that every other networking application on the box worked fine was irrelevant). He spent weeks writing workarounds, which, for some odd reason, didn’t seem to fix the problem. When finally forced to sit down and read the documentation on select, he discovered the problem and corrected it in a matter of minutes. We now use the phrase “select is broken’’ as a gentle reminder whenever one of us starts blaming the system for a fault that is likely to be our own.

Tip 33 “select” Isn’t Broken

Remember, if you see hoof prints, think horses—not zebras. The OS is probably not broken. And select is probably just fine.

If you “changed only one thing’’ and the system stopped working, that one thing was likely to be responsible, directly or indirectly, no matter how farfetched it seems. Sometimes the thing that changed is outside of your control: new versions of the OS, compiler, database, or other third-party software can wreak havoc with previously correct code. New bugs might show up. Bugs for which you had a workaround get fixed, breaking the workaround. APIs change, functionality changes; in short, it’s a whole new ball game, and you must retest the system under these new conditions. So keep a close eye on the schedule when considering an upgrade; you may want to wait until after the next release.


The Element of Surprise


When you find yourself surprised by a bug (perhaps even muttering “that’s impossible” under your breath where we can’t hear you), you must reevaluate truths you hold dear. In that discount calculation algorithm—the one you knew was bulletproof and couldn’t possibly be the cause of this bug—did you test all the boundary conditions? That other piece of code you’ve been using for years—it couldn’t possibly still have a bug in it. Could it?

Of course it can. The amount of surprise you feel when something goes wrong is proportional to the amount of trust and faith you have in the code being run. That’s why, when faced with a “surprising’’ failure, you must accept that one or more of your assumptions is wrong. Don’t gloss over a routine or piece of code involved in the bug because you “know” it works. Prove it. Prove it in this context, with this data, with these boundary conditions.

Tip 34 Don’t Assume It—Prove It

When you come across a surprise bug, beyond merely fixing it, you need to determine why this failure wasn’t caught earlier. Consider whether you need to amend the unit or other tests so that they would have caught it.

Also, if the bug is the result of bad data that was propagated through a couple of levels before causing the explosion, see if better parameter checking in those routines would have isolated it earlier (see the discussions on crashing early and assertions here and here, respectively).

While you’re at it, are there any other places in the code that may be susceptible to this same bug? Now is the time to find and fix them. Make sure that whatever happened, you’ll know if it happens again.

If it took a long time to fix this bug, ask yourself why. Is there anything you can do to make fixing this bug easier the next time around? Perhaps you could build in better testing hooks, or write a log file analyzer.

Finally, if the bug is the result of someone’s wrong assumption, discuss the problem with the whole team: if one person misunderstands, then it’s possible many people do.

Do all this, and hopefully you won’t be surprised next time.


Debugging Checklist


Is the problem being reported a direct result of the underlying bug, or merely a symptom?


Is the bug really in the framework you’re using? Is it in the OS? Or is it in your code?


If you explained this problem in detail to a coworker, what would you say?


If the suspect code passes its unit tests, are the tests complete enough? What happens if you run the tests with this data?


Do the conditions that caused this bug exist anywhere else in the system? Are there other bugs still in the larval stage, just waiting to hatch?


Related Sections Include


Topic 24, ​Dead Programs Tell No Lies​


Challenges


Debugging is challenge enough.


Topic 21

Text Manipulation


Pragmatic Programmers manipulate text the same way woodworkers shape wood. In previous sections we discussed some specific tools—shells, editors, debuggers—that we use. These are similar to a woodworker’s chisels, saws, and planes—tools specialized to do one or two jobs well. However, every now and then we need to perform some transformation not readily handled by the basic tool set. We need a general-purpose text manipulation tool.

Text manipulation languages are to programming what routers[28] are to woodworking. They are noisy, messy, and somewhat brute force. Make mistakes with them, and entire pieces can be ruined. Some people swear they have no place in the toolbox. But in the right hands, both routers and text manipulation languages can be incredibly powerful and versatile. You can quickly trim something into shape, make joints, and carve. Used properly, these tools have surprising finesse and subtlety. But they take time to master.

Fortunately, there are a number of great text manipulation languages. Unix developers (and we include macOS users here) often like to use the power of their command shells, augmented with tools such as awk and sed. People who prefer a more structured tool may prefer languages such as Python or Ruby.

These languages are important enabling technologies. Using them, you can quickly hack up utilities and prototype ideas—jobs that might take five or ten times as long using conventional languages. And that multiplying factor is crucially important to the kind of experimenting that we do. Spending 30 minutes trying out a crazy idea is a whole lot better than spending five hours. Spending a day automating important components of a project is acceptable; spending a week might not be. In their book The Practice of Programming [KP99], Kernighan and Pike built the same program in five different languages. The Perl version was the shortest (17 lines, compared with C’s 150). With Perl you can manipulate text, interact with programs, talk over networks, drive web pages, perform arbitrary precision arithmetic, and write programs that look like Snoopy swearing.

Tip 35 Learn a Text Manipulation Language

To show the wide-ranging applicability of text manipulation languages, here’s a sample of some stuff we’ve done with Ruby and Python just related to the creation of this book:

Building the Book

The build system for the Pragmatic Bookshelf is written in Ruby. Authors, editors, layout people, and support folks use Rake tasks to coordinate the building of PDFs and ebooks.


Code inclusion and highlighting

We think it is important that any code presented in a book should have been tested first. Most of the code in this book has been. However, using the DRY principle (see Topic 9, ​DRY—The Evils of Duplication​) we didn’t want to copy and paste lines of code from the tested programs into the book. That would mean we’d be duplicating code, virtually guaranteeing that we’d forget to update an example when the corresponding program was changed. For some examples, we also didn’t want to bore you with all the framework code needed to make our example compile and run. We turned to Ruby. A relatively simple script is invoked when we format the book—it extracts a named segment of a source file, does syntax highlighting, and converts the result into the typesetting language we use.


Website update

We have a simple script that does a partial book build, extracts the table of contents, then uploads it to the book’s page on our website. We also have a script that extracts sections of a book and uploads them as samples.


Including equations

There’s a Python script that converts LaTeX math markup into nicely formatted text.


Index generation

Most indexes are created as separate documents (which makes maintaining them difficult if a document changes). Ours are marked up in the text itself, and a Ruby script collates and formats the entries.


And so on. In a very real way, the Pragmatic Bookshelf is built around text manipulation. And if you follow our advice to keep things in plain text, then using these languages to manipulate that text will bring a whole host of benefits.


Related Sections Include


Topic 16, ​The Power of Plain Text​

Topic 17, ​Shell Games​


Exercises


Exercise 11

You’re rewriting an application that used to use YAML as a configuration language. Your company has now standardized on JSON, so you have a bunch of .yaml files that need to be turned into .json. Write a script that takes a directory and converts each .yaml file into a corresponding .json file (so database.yaml becomes database.json, and the contents are valid JSON).

Exercise 12

Your team initially chose to use camelCase names for variables, but then changed their collective mind and switched to snake_case. Write a script that scans all the source files for camelCase names and reports on them.

Exercise 13

Following on from the previous exercise, add the ability to change those variable names automatically in one or more files. Remember to keep a backup of the originals in case something goes horribly, horribly wrong.


Topic 22

Engineering Daybooks


Dave once worked for a small computer manufacturer, which meant working alongside electronic and sometimes mechanical engineers.

Many of them walked around with a paper notebook, normally with a pen stuffed down the spine. Every now and then when we were talking, they’d pop the notebook open and scribble something.

Eventually Dave asked the obvious question. It turned out that they’d been trained to keep an engineering daybook, a kind of journal in which they recorded what they did, things they’d learned, sketches of ideas, readings from meters: basically anything to do with their work. When the notebook became full, they’d write the date range on the spine, then stick it on the shelf next to previous daybooks. There may have been a gentle competition going on for whose set of books took the most shelf space.

We use daybooks to take notes in meetings, to jot down what we’re working on, to note variable values when debugging, to leave reminders where we put things, to record wild ideas, and sometimes just to doodle.[29]

The daybook has three main benefits:

It is more reliable than memory. People might ask “What was the name of that company you called last week about the power supply problem?” and you can flip back a page or so and give them the name and number.


It gives you a place to store ideas that aren’t immediately relevant to the task at hand. That way you can continue to concentrate on what you are doing, knowing that the great idea won’t be forgotten.


It acts as a kind of rubber duck (described here). When you stop to write something down, your brain may switch gears, almost as if talking to someone—a great chance to reflect. You may start to make a note and then suddenly realize that what you’d just done, the topic of the note, is just plain wrong.


There’s an added benefit, too. Every now and then you can look back at what you were doing oh-so-many-years-ago and think about the people, the projects, and the awful clothes and hairstyles.

So, try keeping an engineering daybook. Use paper, not a file or a wiki: there’s something special about the act of writing compared to typing. Give it a month, and see if you’re getting any benefits.

If nothing else, it’ll make writing your memoir easier when you’re rich and famous.


Related Sections Include


Topic 6, ​Your Knowledge Portfolio​

Topic 37, ​Listen to Your Lizard Brain​


Footnotes


[24]

All software becomes legacy software as soon as it’s written.


[25]

Although the Elm language does have a time-traveling debugger.


[26]

Why “rubber ducking’’? While an undergraduate at Imperial College in London, Dave did a lot of work with a research assistant named Greg Pugh, one of the best developers Dave has known. For several months Greg carried around a small yellow rubber duck, which he’d place on his terminal while coding. It was a while before Dave had the courage to ask….


[27]

Earlier versions of the book talked about talking to your pot plant. It was a typo. Honest.


[28]

Here router means the tool that spins cutting blades very, very fast, not a device for interconnecting networks.


[29]

There is some evidence that doodling helps focus and improves cognitive skills, for example, see What does doodling do? [And10].





Chapter 4


Pragmatic Paranoia


Tip 36 You Can’t Write Perfect Software

Did that hurt? It shouldn’t. Accept it as an axiom of life. Embrace it. Celebrate it. Because perfect software doesn’t exist. No one in the brief history of computing has ever written a piece of perfect software. It’s unlikely that you’ll be the first. And unless you accept this as a fact, you’ll end up wasting time and energy chasing an impossible dream.

So, given this depressing reality, how does a Pragmatic Programmer turn it into an advantage? That’s the topic of this chapter.

Everyone knows that they personally are the only good driver on Earth. The rest of the world is out there to get them, blowing through stop signs, weaving between lanes, not indicating turns, texting on the phone, and just generally not living up to our standards. So we drive defensively. We look out for trouble before it happens, anticipate the unexpected, and never put ourselves into a position from which we can’t extricate ourselves.

The analogy with coding is pretty obvious. We are constantly interfacing with other people’s code—code that might not live up to our high standards—and dealing with inputs that may or may not be valid. So we are taught to code defensively. If there’s any doubt, we validate all information we’re given. We use assertions to detect bad data, and distrust data from potential attackers or trolls. We check for consistency, put constraints on database columns, and generally feel pretty good about ourselves.

But Pragmatic Programmers take this a step further. They don’t trust themselves, either. Knowing that no one writes perfect code, including themselves, Pragmatic Programmers build in defenses against their own mistakes. We describe the first defensive measure in Topic 23, ​Design by Contract​: clients and suppliers must agree on rights and responsibilities.

In Topic 24, ​Dead Programs Tell No Lies​, we want to ensure that we do no damage while we’re working the bugs out. So we try to check things often and terminate the program if things go awry.

Topic 25, ​Assertive Programming​ describes an easy method of checking along the way—write code that actively verifies your assumptions.

As your programs get more dynamic, you’ll find yourself juggling system resources—memory, files, devices, and the like. In Topic 26, ​How to Balance Resources​, we’ll suggest ways of ensuring that you don’t drop any of the balls.

And most importantly, we stick to small steps always, as described in Topic 27, ​Don’t Outrun Your Headlights​, so we don’t fall off the edge of the cliff.

In a world of imperfect systems, ridiculous time scales, laughable tools, and impossible requirements, let’s play it safe. As Woody Allen said, “When everybody actually is out to get you, paranoia is just good thinking.”


Topic 23

Design by Contract


Nothing astonishes men so much as common sense and plain dealing.


Ralph Waldo Emerson, Essays


Dealing with computer systems is hard. Dealing with people is even harder. But as a species, we’ve had longer to figure out issues of human interactions. Some of the solutions we’ve come up with during the last few millennia can be applied to writing software as well. One of the best solutions for ensuring plain dealing is the contract.

A contract defines your rights and responsibilities, as well as those of the other party. In addition, there is an agreement concerning repercussions if either party fails to abide by the contract.

Maybe you have an employment contract that specifies the hours you’ll work and the rules of conduct you must follow. In return, the company pays you a salary and other perks. Each party meets its obligations and everyone benefits.

It’s an idea used the world over—both formally and informally—to help humans interact. Can we use the same concept to help software modules interact? The answer is “yes.’’


DBC


Bertrand Meyer (Object-Oriented Software Construction [Mey97]) developed the concept of Design by Contract for the language Eiffel.[30] It is a simple yet powerful technique that focuses on documenting (and agreeing to) the rights and responsibilities of software modules to ensure program correctness. What is a correct program? One that does no more and no less than it claims to do. Documenting and verifying that claim is the heart of Design by Contract (DBC, for short).

Every function and method in a software system does something. Before it starts that something, the function may have some expectation of the state of the world, and it may be able to make a statement about the state of the world when it concludes. Meyer describes these expectations and claims as follows:

Preconditions

What must be true in order for the routine to be called; the routine’s requirements. A routine should never get called when its preconditions would be violated. It is the caller’s responsibility to pass good data (see the box here).


Postconditions

What the routine is guaranteed to do; the state of the world when the routine is done. The fact that the routine has a postcondition implies that it will conclude: infinite loops aren’t allowed.


Class invariants

A class ensures that this condition is always true from the perspective of a caller. During internal processing of a routine, the invariant may not hold, but by the time the routine exits and control returns to the caller, the invariant must be true. (Note that a class cannot give unrestricted write-access to any data member that participates in the invariant.)


The contract between a routine and any potential caller can thus be read as

If all the routine’s preconditions are met by the caller, the routine shall guarantee that all postconditions and invariants will be true when it completes.


If either party fails to live up to the terms of the contract, then a remedy (which was previously agreed to) is invoked—maybe an exception is raised, or the program terminates. Whatever happens, make no mistake that failure to live up to the contract is a bug. It is not something that should ever happen, which is why preconditions should not be used to perform things such as user-input validation.

Some languages have better support for these concepts than others. Clojure, for example, supports pre- and post-conditions as well as the more comprehensive instrumentation provided by specs. Here’s an example of a banking function to make a deposit using simple pre- and post-conditions:

​ (​defn​ accept-deposit [account-id amount]

​ { ​:pre​ [ (> amount 0.00)

​ (account-open? account-id) ]

​ ​:post​ [ (contains? (account-transactions account-id) %) ] }

​ ​"Accept a deposit and return the new transaction id"​

​ ​;; Some other processing goes here...​

​ ​;; Return the newly created transaction:​

​ (create-transaction account-id :deposit amount))

There are two preconditions for the accept-deposit function. The first is that the amount is greater than zero, and the second is that the account is open and valid, as determined by some function named account-open?. There is also a postcondition: the function guarantees that the new transaction (the return value of this function, represented here by ‘%’) can be found among the transactions for this account.

If you call accept-deposit with a positive amount for the deposit and a valid account, it will proceed to create a transaction of the appropriate type and do whatever other processing it does. However, if there’s a bug in the program and you somehow passed in a negative amount for the deposit, you’ll get a runtime exception:

​ Exception in thread "main"...

​ Caused by: java.lang.AssertionError: Assert failed: (> amount 0.0)

Similarly, this function requires that the specified account is open and valid. If it’s not, you’ll see that exception instead:

​ Exception in thread "main"...

​ Caused by: java.lang.AssertionError: Assert failed: (account-open? account-id)

Other languages have features that, while not DBC-specific, can still be used to good effect. For example, Elixir uses guard clauses to dispatch function calls against several available bodies:

​ ​defmodule​ Deposits ​do​

​ ​def​ accept_deposit(account_id, amount) ​when​ (amount > 100000) ​do​

​ ​# Call the manager!​

​ ​end​

​ ​def​ accept_deposit(account_id, amount) ​when​ (amount > 10000) ​do​

​ ​# Extra Federal requirements for reporting​

​ ​# Some processing...​

​ ​end​

​ ​def​ accept_deposit(account_id, amount) ​when​ (amount > 0) ​do​

​ ​# Some processing...​

​ ​end​

​ ​end​

In this case, calling accept_deposit with a large enough amount may trigger additional steps and processing. Try to call it with an amount less than or equal to zero, however, and you’ll get an exception informing you that you can’t:

​ ** (FunctionClauseError) no function clause matching in Deposits.accept_deposit/2

This is a better approach than simply checking your inputs; in this case, you simply can not call this function if your arguments are out of range.

Tip 37 Design with Contracts

In Topic 10, ​Orthogonality​, we recommended writing “shy” code. Here, the emphasis is on “lazy” code: be strict in what you will accept before you begin, and promise as little as possible in return. Remember, if your contract indicates that you’ll accept anything and promise the world in return, then you’ve got a lot of code to write!

In any programming language, whether it’s functional, object-oriented, or procedural, DBC forces you to think.


DBC and Test-Driven Development

Is Design by Contract needed in a world where developers practice unit testing, test-driven development (TDD), property-based testing, or defensive programming?

The short answer is “yes.”

DBC and testing are different approaches to the broader topic of program correctness. They both have value and both have uses in different situations. DBC offers several advantages over specific testing approaches:

DBC doesn’t require any setup or mocking


DBC defines the parameters for success or failure in all cases, whereas testing can only target one specific case at a time


TDD and other testing happens only at “test time” within the build cycle. But DBC and assertions are forever: during design, development, deployment, and maintenance


TDD does not focus on checking internal invariants within the code under test, it’s more black-box style to check the public interface


DBC is more efficient (and DRY-er) than defensive programming, where everyone has to validate data in case no one else does.


TDD is a great technique, but as with many techniques, it might invite you to concentrate on the “happy path,” and not the real world full of bad data, bad actors, bad versions, and bad specifications.


Class Invariants and Functional Languages


It’s a naming thing. Eiffel is an object-oriented language, so Meyer named this idea “class invariant.” But, really, it’s more general than that. What this idea really refers to is state. In an object-oriented language, the state is associated with instances of classes. But other languages have state, too.

In a functional language, you typically pass state to functions and receive updated state as a result. The concepts of invariants is just as useful in these circumstances.


Implementing DBC


Simply enumerating what the input domain range is, what the boundary conditions are, and what the routine promises to deliver—or, more importantly, what it doesn’t promise to deliver—before you write the code is a huge leap forward in writing better software. By not stating these things, you are back to programming by coincidence (see the discussion here), which is where many projects start, finish, and fail.

In languages that do not support DBC in the code, this might be as far as you can go—and that’s not too bad. DBC is, after all, a design technique. Even without automatic checking, you can put the contract in the code as comments or in the unit tests and still get a very real benefit.


Assertions


While documenting these assumptions is a great start, you can get much greater benefit by having the compiler check your contract for you. You can partially emulate this in some languages by using assertions: runtime checks of logical conditions (see Topic 25, ​Assertive Programming​). Why only partially? Can’t you use assertions to do everything DBC can do?

Unfortunately, the answer is no. To begin with, in object-oriented languages there probably is no support for propagating assertions down an inheritance hierarchy. This means that if you override a base class method that has a contract, the assertions that implement that contract will not be called correctly (unless you duplicate them manually in the new code). You must remember to call the class invariant (and all base class invariants) manually before you exit every method. The basic problem is that the contract is not automatically enforced.

In other environments, the exceptions generated from DBC-style assertions might be turned off globally or ignored in the code.

Also, there is no built-in concept of “old’’ values; that is, values as they existed at the entry to a method. If you’re using assertions to enforce contracts, you must add code to the precondition to save any information you’ll want to use in the postcondition, if the language will even allow that. In the Eiffel language, where DBC was born, you can just use old expression.

Finally, conventional runtime systems and libraries are not designed to support contracts, so these calls are not checked. This is a big loss, because it is often at the boundary between your code and the libraries it uses that the most problems are detected (see Topic 24, ​Dead Programs Tell No Lies​ for a more detailed discussion).


Who's Responsible?

Who is responsible for checking the precondition, the caller or the routine being called? When implemented as part of the language, the answer is neither: the precondition is tested behind the scenes after the caller invokes the routine but before the routine itself is entered. Thus if there is any explicit checking of parameters to be done, it must be performed by the caller, because the routine itself will never see parameters that violate its precondition. (For languages without built-in support, you would need to bracket the called routine with a preamble and/or postamble that checks these assertions.)

Consider a program that reads a number from the console, calculates its square root (by calling sqrt), and prints the result. The sqrt function has a precondition—its argument must not be negative. If the user enters a negative number at the console, it is up to the calling code to ensure that it never gets passed to sqrt. This calling code has many options: it could terminate, it could issue a warning and read another number, or it could make the number positive and append an i to the result returned by sqrt. Whatever its choice, this is definitely not sqrt’s problem.

By expressing the domain of the square root function in the precondition of the sqrt routine, you shift the burden of correctness to the caller—where it belongs. You can then design the sqrt routine secure in the knowledge that its input will be in range.


DBC and Crashing Early


DBC fits in nicely with our concept of crashing early (see Topic 24, ​Dead Programs Tell No Lies​). By using an assert or DBC mechanism to validate the preconditions, postconditions, and invariants, you can crash early and report more accurate information about the problem.

For example, suppose you have a method that calculates square roots. It needs a DBC precondition that restricts the domain to positive numbers. In languages that support DBC, if you pass sqrt a negative parameter, you’ll get an informative error such as sqrt_arg_must_be_positive, along with a stack trace.

This is better than the alternative in other languages such as Java, C, and C++ where passing a negative number to sqrt returns the special value NaN (Not a Number). It may be some time later in the program that you attempt to do some math on NaN, with surprising results.

It’s much easier to find and diagnose the problem by crashing early, at the site of the problem.


Semantic Invariants


You can use semantic invariants to express inviolate requirements, a kind of “philosophical contract.’’

We once wrote a debit card transaction switch. A major requirement was that the user of a debit card should never have the same transaction applied to their account twice. In other words, no matter what sort of failure mode might happen, the error should be on the side of not processing a transaction rather than processing a duplicate transaction.

This simple law, driven directly from the requirements, proved to be very helpful in sorting out complex error recovery scenarios, and guided the detailed design and implementation in many areas.

Be sure not to confuse requirements that are fixed, inviolate laws with those that are merely policies that might change with a new management regime. That’s why we use the term semantic invariants—it must be central to the very meaning of a thing, and not subject to the whims of policy (which is what more dynamic business rules are for).

When you find a requirement that qualifies, make sure it becomes a well-known part of whatever documentation you are producing—whether it is a bulleted list in the requirements document that gets signed in triplicate or just a big note on the common whiteboard that everyone sees. Try to state it clearly and unambiguously. For example, in the debit card example, we might write

Err in favor of the consumer.


This is a clear, concise, unambiguous statement that’s applicable in many different areas of the system. It is our contract with all users of the system, our guarantee of behavior.


Dynamic Contracts and Agents


Until now, we have talked about contracts as fixed, immutable specifications. But in the landscape of autonomous agents, this doesn’t need to be the case. By the definition of “autonomous,” agents are free to reject requests that they do not want to honor. They are free to renegotiate the contract—“I can’t provide that, but if you give me this, then I might provide something else.”

Certainly any system that relies on agent technology has a critical dependence on contractual arrangements—even if they are dynamically generated.

Imagine: with enough components and agents that can negotiate their own contracts among themselves to achieve a goal, we might just solve the software productivity crisis by letting software solve it for us.

But if we can’t use contracts by hand, we won’t be able to use them automatically. So next time you design a piece of software, design its contract as well.


Related Sections Include


Topic 24, ​Dead Programs Tell No Lies​

Topic 25, ​Assertive Programming​

Topic 38, ​Programming by Coincidence​

Topic 42, ​Property-Based Testing​

Topic 43, ​Stay Safe Out There​

Topic 45, ​The Requirements Pit​


Challenges


Points to ponder: If DBC is so powerful, why isn’t it used more widely? Is it hard to come up with the contract? Does it make you think about issues you’d rather ignore for now? Does it force you to THINK!? Clearly, this is a dangerous tool!


Exercises


Exercise 14 (possible answer)

Design an interface to a kitchen blender. It will eventually be a web-based, IoT-enabled blender, but for now we just need the interface to control it. It has ten speed settings (0 means off). You can’t operate it empty, and you can change the speed only one unit at a time (that is, from 0 to 1, and from 1 to 2, not from 0 to 2).

Here are the methods. Add appropriate pre- and postconditions and an invariant.

​ int getSpeed()

​ void setSpeed(int x)

​ boolean isFull()

​ void fill()

​ void empty()

Exercise 15 (possible answer)

How many numbers are in the series 0, 5, 10, 15, …, 100?


Topic 24

Dead Programs Tell No Lies


Have you noticed that sometimes other people can detect that things aren’t well with you before you’re aware of the problem yourself? It’s the same with other people’s code. If something is starting to go awry with one of our programs, sometimes it is a library or framework routine that catches it first. Maybe we’ve passed in a nil value, or an empty list. Maybe there’s a missing key in that hash, or the value we thought contained a hash really contains a list instead. Maybe there was a network error or filesystem error that we didn’t catch, and we’ve got empty or corrupted data. A logic error a couple of million instructions ago means that the selector for a case statement is no longer the expected 1, 2, or 3. We’ll hit the default case unexpectedly. That’s also one reason why each and every case/switch statement needs to have a default clause: we want to know when the “impossible” has happened.

It’s easy to fall into the “it can’t happen” mentality. Most of us have written code that didn’t check that a file closed successfully, or that a trace statement got written as we expected. And all things being equal, it’s likely that we didn’t need to—the code in question wouldn’t fail under any normal conditions. But we’re coding defensively. We’re making sure that the data is what we think it is, that the code in production is the code we think it is. We’re checking that the correct versions of dependencies were actually loaded.

All errors give you information. You could convince yourself that the error can’t happen, and choose to ignore it. Instead, Pragmatic Programmers tell themselves that if there is an error, something very, very bad has happened. Don’t forget to Read the Damn Error Message (see ​Coder in a Strange Land​).


Catch and Release Is for Fish


Some developers feel that is it good style to catch or rescue all exceptions, re-raising them after writing some kind of message. Their code is full of things like this (where a bare raise statement reraises the current exception):

​ ​try​ ​do​

​ add_score_to_board(score);

​ ​rescue​ InvalidScore

​ Logger.error(​"​​Can't add invalid score. Exiting"​);

​ ​raise​

​ ​rescue​ BoardServerDown

​ Logger.error(​"​​Can't add score: board is down. Exiting"​);

​ ​raise​

​ ​rescue​ StaleTransaction

​ Logger.error(​"​​Can't add score: stale transaction. Exiting"​);

​ ​raise​

​ ​end​

Here’s how Pragmatic Programmers would write this:

​ add_score_to_board(score);

We prefer it for two reasons. First, the application code isn’t eclipsed by the error handling. Second, and perhaps more important, the code is less coupled. In the verbose example, we have to list every exception the add_score_to_board method could raise. If the writer of that method adds another exception, our code is subtly out of date. In the more pragmatic second version, the new exception is automatically propagated.

Tip 38 Crash Early


Crash, Don’t Trash


One of the benefits of detecting problems as soon as you can is that you can crash earlier, and crashing is often the best thing you can do. The alternative may be to continue, writing corrupted data to some vital database or commanding the washing machine into its twentieth consecutive spin cycle.

The Erlang and Elixir languages embrace this philosophy. Joe Armstrong, inventor of Erlang and author of Programming Erlang: Software for a Concurrent World [Arm07], is often quoted as saying, “Defensive programming is a waste of time. Let it crash!” In these environments, programs are designed to fail, but that failure is managed with supervisors. A supervisor is responsible for running code and knows what to do in case the code fails, which could include cleaning up after it, restarting it, and so on. What happens when the supervisor itself fails? Its own supervisor manages that event, leading to a design composed of supervisor trees. The technique is very effective and helps to account for the use of these languages in high-availability, fault-tolerant systems.

In other environments, it may be inappropriate simply to exit a running program. You may have claimed resources that might not get released, or you may need to write log messages, tidy up open transactions, or interact with other processes.

However, the basic principle stays the same—when your code discovers that something that was supposed to be impossible just happened, your program is no longer viable. Anything it does from this point forward becomes suspect, so terminate it as soon as possible.

A dead program normally does a lot less damage than a crippled one.


Related Sections Include


Topic 20, ​Debugging​

Topic 23, ​Design by Contract​

Topic 25, ​Assertive Programming​

Topic 26, ​How to Balance Resources​

Topic 43, ​Stay Safe Out There​


Topic 25

Assertive Programming


There is a luxury in self-reproach. When we blame ourselves we feel no one else has a right to blame us.


Oscar Wilde, The Picture of Dorian Gray


It seems that there’s a mantra that every programmer must memorize early in his or her career. It is a fundamental tenet of computing, a core belief that we learn to apply to requirements, designs, code, comments, just about everything we do. It goes

This can never happen…


“This application will never be used abroad, so why internationalize it?” “count can’t be negative.” “Logging can’t fail.”

Let’s not practice this kind of self-deception, particularly when coding.

Tip 39 Use Assertions to Prevent the Impossible

Whenever you find yourself thinking “but of course that could never happen,” add code to check it. The easiest way to do this is with assertions. In many language implementations, you’ll find some form of assert that checks a Boolean condition.[31] These checks can be invaluable. If a parameter or a result should never be null, then check for it explicitly:

​ ​assert​ (result != ​null​);

In the Java implementation, you can (and should) add a descriptive string:

​ ​assert​ result != ​null​ && result.size() > 0 : ​"Empty result from XYZ"​;

Assertions are also useful checks on an algorithm’s operation. Maybe you’ve written a clever sort algorithm, named my_sort. Check that it works:

​ books = my_sort(find(​"scifi"​))

​ assert(is_sorted?(books))

Don’t use assertions in place of real error handling. Assertions check for things that should never happen: you don’t want to be writing code such as the following:

​ puts(​"Enter 'Y' or 'N': "​)

​ ans = gets[0] ​# Grab first character of response​

​ assert((ch == ​'Y'​) || (ch == ​'N'​)) ​# Very bad idea!​

And just because most assert implementations will terminate the process when an assertion fails, there’s no reason why versions you write should. If you need to free resources, catch the assertion’s exception or trap the exit, and run your own error handler. Just make sure the code you execute in those dying milliseconds doesn’t rely on the information that triggered the assertion failure in the first place.


Assertions and Side Effects


It’s embarrassing when the code we add to detect errors actually ends up creating new errors. This can happen with assertions if evaluating the condition has side effects. For example, it would be a bad idea to code something such as

​ ​while​ (iter.hasMoreElements()) {

​ ​assert​(iter.nextElement() != ​null​);

​ Object obj = iter.nextElement();

​ ​// ....​

​ }

The .nextElement() call in the assertion has the side effect of moving the iterator past the element being fetched, and so the loop will process only half the elements in the collection. It would be better to write

​ ​while​ (iter.hasMoreElements()) {

​ Object obj = iter.nextElement();

​ ​assert​(obj != ​null​);

​ ​// ....​

​ }

This problem is a kind of Heisenbug[32]—debugging that changes the behavior of the system being debugged.

(We also believe that nowadays, when most languages have decent support for iterating functions over collections, this kind of explicit loop is unnecessary and bad form.)


Leave Assertions Turned On


There is a common misunderstanding about assertions. It goes something like this:

Assertions add some overhead to code. Because they check for things that should never happen, they’ll get triggered only by a bug in the code. Once the code has been tested and shipped, they are no longer needed, and should be turned off to make the code run faster. Assertions are a debugging facility.


There are two patently wrong assumptions here. First, they assume that testing finds all the bugs. In reality, for any complex program you are unlikely to test even a minuscule percentage of the permutations your code will be put through. Second, the optimists are forgetting that your program runs in a dangerous world. During testing, rats probably won’t gnaw through a communications cable, someone playing a game won’t exhaust memory, and log files won’t fill the storage partition. These things might happen when your program runs in a production environment. Your first line of defense is checking for any possible error, and your second is using assertions to try to detect those you’ve missed.

Turning off assertions when you deliver a program to production is like crossing a high wire without a net because you once made it across in practice. There’s dramatic value, but it’s hard to get life insurance.

Even if you do have performance issues, turn off only those assertions that really hit you. The sort example above may be a critical part of your application, and may need to be fast. Adding the check means another pass through the data, which might be unacceptable. Make that particular check optional, but leave the rest in.


Use Assertions in Production, Win Big Money

A former neighbor of Andy’s headed up a small startup company that made network devices. One of their secrets to success was the decision to leave assertions in place in production releases. These assertions were well crafted to report all the pertinent data leading to the failure, and presented via a nice-looking UI to the end user. This level of feedback, from real users under actual conditions, allowed the developers to plug the holes and fix these obscure, hard-to-reproduce bugs, resulting in remarkably stable, bullet-proof software.

This small, unknown company had such a solid product, it was soon acquired for hundreds of millions of dollars.

Just sayin’.


Exercise 16 (possible answer)

A quick reality check. Which of these “impossible” things can happen?

A month with fewer than 28 days

Error code from a system call: can’t access the current directory

In C++: a = 2; b = 3; but (a + b) does not equal 5

A triangle with an interior angle sum ≠ 180°

A minute that doesn’t have 60 seconds

(a + 1) <= a


Related Sections Include


Topic 23, ​Design by Contract​

Topic 24, ​Dead Programs Tell No Lies​

Topic 42, ​Property-Based Testing​

Topic 43, ​Stay Safe Out There​


Topic 26

How to Balance Resources


To light a candle is to cast a shadow...


Ursula K. Le Guin, A Wizard of Earthsea


We all manage resources whenever we code: memory, transactions, threads, network connections, files, timers—all kinds of things with limited availability. Most of the time, resource usage follows a predictable pattern: you allocate the resource, use it, and then deallocate it.

However, many developers have no consistent plan for dealing with resource allocation and deallocation. So let us suggest a simple tip:

Tip 40 Finish What You Start

This tip is easy to apply in most circumstances. It simply means that the function or object that allocates a resource should be responsible for deallocating it. Let’s see how it applies by looking at an example of some bad code—part of a Ruby program that opens a file, reads customer information from it, updates a field, and writes the result back. We’ve eliminated error handling to make the example clearer:

​ ​def​ read_customer

​ @customer_file = File.open(@name + ​".rec"​, ​"r+"​)

​ @balance = BigDecimal(@customer_file.gets)

​ ​end​

​

​ ​def​ write_customer

​ @customer_file.rewind

​ @customer_file.puts @balance.to_s

​ @customer_file.close

​ ​end​

​

​ ​def​ update_customer(transaction_amount)

​ read_customer

​ @balance = @balance.add(transaction_amount,2)

​ write_customer

​ ​end​

At first sight, the routine update_customer looks reasonable. It seems to implement the logic we require—reading a record, updating the balance, and writing the record back out. However, this tidiness hides a major problem. The routines read_customer and write_customer are tightly coupled[33]—they share the instance variable customer_file. read_customer opens the file and stores the file reference in customer_file, and then write_customer uses that stored reference to close the file when it finishes. This shared variable doesn’t even appear in the update_customer routine.

Why is this bad? Let’s consider the unfortunate maintenance programmer who is told that the specification has changed—the balance should be updated only if the new value is not negative. They go into the source and change update_customer:

​ ​def​ update_customer(transaction_amount)

​ read_customer

​ ​if​ (transaction_amount >= 0.00)

​ @balance = @balance.add(transaction_amount,2)

​ write_customer

​ ​end​

​ ​end​

All seems fine during testing. However, when the code goes into production, it collapses after several hours, complaining of too many open files. It turns out that write_customer is not getting called in some circumstances. When that happens, the file is not getting closed.

A very bad solution to this problem would be to deal with the special case in update_customer:.

​ ​def​ update_customer(transaction_amount)

​ read_customer

​ ​if​ (transaction_amount >= 0.00)

​ @balance += BigDecimal(transaction_amount, 2)

​ write_customer

​ ​else​

​ @customer_file.close ​# Bad idea!​

​ ​end​

​ ​end​

This will fix the problem—the file will now get closed regardless of the new balance—but the fix now means that three routines are coupled through the shared variable customer_file, and keeping track of when the file is open or not is going to start to get messy. We’re falling into a trap, and things are going to start going downhill rapidly if we continue on this course. This is not balanced!

The finish what you start tip tells us that, ideally, the routine that allocates a resource should also free it. We can apply it here by refactoring the code slightly:

​ ​def​ read_customer(file)

​ @balance=BigDecimal(file.gets)

​ ​end​

​

​ ​def​ write_customer(file)

​ file.rewind

​ file.puts @balance.to_s

​ ​end​

​

​ ​def​ update_customer(transaction_amount)

​ file=File.open(@name + ​".rec"​, ​"r+"​) ​# >--​

​ read_customer(file) ​# |​

​ @balance = @balance.add(transaction_amount,2) ​# |​

​ file.close ​# <--​

​ ​end​

Instead of holding on to the file reference, we’ve changed the code to pass it as a parameter.[34] Now all the responsibility for the file is in the update_customer routine. It opens the file and (finishing what it starts) closes it before returning. The routine balances the use of the file: the open and close are in the same place, and it is apparent that for every open there will be a corresponding close. The refactoring also removes an ugly shared variable.

There’s another small but important improvement we can make. In many modern languages, you can scope the lifetime of a resource to an enclosed block of some sort. In Ruby, there’s a variation of the file open that passes in the open file reference to a block, shown here between the do and the end:

​ ​def​ update_customer(transaction_amount)

​ File.open(@name + ​".rec"​, ​"r+"​) ​do​ |file| ​# >--​

​ read_customer(file) ​# |​

​ @balance = @balance.add(transaction_amount,2) ​# |​

​ write_customer(file) ​# |​

​ ​end​ ​# <--​

​ ​end​

In this case, at the end of the block the file variable goes out of scope and the external file is closed. Period. No need to remember to close the file and release the source, it is guaranteed to happen for you.

When in doubt, it always pays to reduce scope.

Tip 41 Act Locally


Balancing Over Time

In this topic we’re mostly looking at ephemeral resources used by your running process. But you might want to consider what other messes you might be leaving behind.

For instance, how are your logging files handled? You are creating data and using up storage space. Is there something in place to rotate the logs and clean them up? How about for your unofficial debug files you’re dropping? If you’re adding logging records in a database, is there a similar process in place to expire them? For anything that you create that takes up a finite resource, consider how to balance it.

What else are you leaving behind?


Nest Allocations


The basic pattern for resource allocation can be extended for routines that need more than one resource at a time. There are just two more suggestions:

Deallocate resources in the opposite order to that in which you allocate them. That way you won’t orphan resources if one resource contains references to another.


When allocating the same set of resources in different places in your code, always allocate them in the same order. This will reduce the possibility of deadlock. (If process A claims resource1 and is about to claim resource2, while process B has claimed resource2 and is trying to get resource1, the two processes will wait forever.)


It doesn’t matter what kind of resources we’re using—transactions, network connections, memory, files, threads, windows—the basic pattern applies: whoever allocates a resource should be responsible for deallocating it. However, in some languages we can develop the concept further.


Objects and Exceptions


The equilibrium between allocations and deallocations is reminiscent of an object-oriented class’s constructor and destructor. The class represents a resource, the constructor gives you a particular object of that resource type, and the destructor removes it from your scope.

If you are programming in an object-oriented language, you may find it useful to encapsulate resources in classes. Each time you need a particular resource type, you instantiate an object of that class. When the object goes out of scope, or is reclaimed by the garbage collector, the object’s destructor then deallocates the wrapped resource.

This approach has particular benefits when you’re working with languages where exceptions can interfere with resource deallocation.


Balancing and Exceptions


Languages that support exceptions can make resource deallocation tricky. If an exception is thrown, how do you guarantee that everything allocated prior to the exception is tidied up? The answer depends to some extent on the language support. You generally have two choices:

Use variable scope (for example, stack variables in C++ or Rust)

Use a finally clause in a try…catch block


With usual scoping rules in languages such as C++ or Rust, the variable’s memory will be reclaimed when the variable goes out of scope via a return, block exit, or exception. But you can also hook in to the variable’s destructor to cleanup any external resources. In this example, the Rust variable named accounts will automatically close the associated file when it goes out of scope:

​ {

​ ​let​ ​mut​ accounts = File::open(​"mydata.txt"​)?; ​// >--​

​ ​// use 'accounts' // |​

​ ... ​// |​

​ } ​// <--​

​ ​// 'accounts' is now out of scope, and the file is​

​ ​// automatically closed​

The other option, if the language supports it, is the finally clause. A finally clause will ensure that the specified code will run whether or not an exception was raised in the try…catch block:

​ ​try​

​ ​// some dodgy stuff​

​ ​catch​

​ ​// exception was raised​

​ ​finally​

​ ​// clean up in either case​

However, there is a catch.


An Exception Antipattern


We commonly see folks writing something like this:

​ begin

​ thing = allocate_resource()

​ process(thing)

​ finally

​ deallocate(thing)

​ end

Can you see what’s wrong?

What happens if the resource allocation fails and raises an exception? The finally clause will catch it, and try to deallocate a thing that was never allocated.

The correct pattern for handling resource deallocation in an environment with exceptions is

​ thing = allocate_resource()

​ begin

​ process(thing)

​ finally

​ deallocate(thing)

​ end


When You Can’t Balance Resources


There are times when the basic resource allocation pattern just isn’t appropriate. Commonly this is found in programs that use dynamic data structures. One routine will allocate an area of memory and link it into some larger structure, where it may stay for some time.

The trick here is to establish a semantic invariant for memory allocation. You need to decide who is responsible for data in an aggregate data structure. What happens when you deallocate the top-level structure? You have three main options:

The top-level structure is also responsible for freeing any substructures that it contains. These structures then recursively delete data they contain, and so on.


The top-level structure is simply deallocated. Any structures that it pointed to (that are not referenced elsewhere) are orphaned.


The top-level structure refuses to deallocate itself if it contains any substructures.


The choice here depends on the circumstances of each individual data structure. However, you need to make it explicit for each, and implement your decision consistently. Implementing any of these options in a procedural language such as C can be a problem: data structures themselves are not active. Our preference in these circumstances is to write a module for each major structure that provides standard allocation and deallocation facilities for that structure. (This module can also provide facilities such as debug printing, serialization, deserialization, and traversal hooks.)


Checking the Balance


Because Pragmatic Programmers trust no one, including ourselves, we feel that it is always a good idea to build code that actually checks that resources are indeed freed appropriately. For most applications, this normally means producing wrappers for each type of resource, and using these wrappers to keep track of all allocations and deallocations. At certain points in your code, the program logic will dictate that the resources will be in a certain state: use the wrappers to check this. For example, a long-running program that services requests will probably have a single point at the top of its main processing loop where it waits for the next request to arrive. This is a good place to ensure that resource usage has not increased since the last execution of the loop.

At a lower, but no less useful level, you can invest in tools that (among other things) check your running programs for memory leaks.


Related Sections Include


Topic 24, ​Dead Programs Tell No Lies​

Topic 30, ​Transforming Programming​

Topic 33, ​Breaking Temporal Coupling​


Challenges


Although there are no guaranteed ways of ensuring that you always free resources, certain design techniques, when applied consistently, will help. In the text we discussed how establishing a semantic invariant for major data structures could direct memory deallocation decisions. Consider how Topic 23, ​Design by Contract​, could help refine this idea.


Exercise 17 (possible answer)

Some C and C++ developers make a point of setting a pointer to NULL after they deallocate the memory it references. Why is this a good idea?

Exercise 18 (possible answer)

Some Java developers make a point of setting an object variable to NULL after they have finished using the object. Why is this a good idea?


Topic 27

Don’t Outrun Your Headlights


	It’s tough to make predictions, especially about the future.


Lawrence "Yogi" Berra, after a Danish Proverb


It’s late at night, dark, pouring rain. The two-seater whips around the tight curves of the twisty little mountain roads, barely holding the corners. A hairpin comes up and the car misses it, crashing though the skimpy guardrail and soaring to a fiery crash in the valley below. State troopers arrive on the scene, and the senior officer sadly shakes their head. “Must have outrun their headlights.”

Had the speeding two-seater been going faster than the speed of light? No, that speed limit is firmly fixed. What the officer referred to was the driver’s ability to stop or steer in time in response to the headlight’s illumination.

Headlights have a certain limited range, known as the throw distance. Past that point, the light spread is too diffuse to be effective. In addition, headlights only project in a straight line, and won’t illuminate anything off-axis, such as curves, hills, or dips in the road. According to the National Highway Traffic Safety Administration, the average distance illuminated by low-beam headlights is about 160 feet. Unfortunately, stopping distance at 40mph is 189 feet, and at 70mph a whopping 464 feet.[35] So indeed, it’s actually pretty easy to outrun your headlights.

In software development, our “headlights” are similarly limited. We can’t see too far ahead into the future, and the further off-axis you look, the darker it gets. So Pragmatic Programmers have a firm rule:

Tip 42 Take Small Steps—Always

Always take small, deliberate steps, checking for feedback and adjusting before proceeding. Consider that the rate of feedback is your speed limit. You never take on a step or a task that’s “too big.”

What do we mean exactly by feedback? Anything that independently confirms or disproves your action. For example:

Results in a REPL provide feedback on your understanding of APIs and algorithms

Unit tests provide feedback on your last code change

User demo and conversation provide feedback on features and usability


What’s a task that’s too big? Any task that requires “fortune telling.” Just as the car headlights have limited throw, we can only see into the future perhaps one or two steps, maybe a few hours or days at most. Beyond that, you can quickly get past educated guess and into wild speculation. You might find yourself slipping into fortune telling when you have to:

Estimate completion dates months in the future

Plan a design for future maintenance or extendability

Guess user’s future needs

Guess future tech availability


But, we hear you cry, aren’t we supposed to design for future maintenance? Yes, but only to a point: only as far ahead as you can see. The more you have to predict what the future will look like, the more risk you incur that you’ll be wrong. Instead of wasting effort designing for an uncertain future, you can always fall back on designing your code to be replaceable. Make it easy to throw out your code and replace it with something better suited. Making code replaceable will also help with cohesion, coupling, and DRY, leading to a better design overall.

Even though you may feel confident of the future, there’s always the chance of a black swan around the corner.


Black Swans


In his book, The Black Swan: The Impact of the Highly Improbable [Tal10], Nassim Nicholas Taleb posits that all significant events in history have come from high-profile, hard-to-predict, and rare events that are beyond the realm of normal expectations. These outliers, while statistically rare, have disproportionate effects. In addition, our own cognitive biases tend to blind us to changes creeping up on the edges of our work (see Topic 4, ​Stone Soup and Boiled Frogs​).

Around the time of the first edition of The Pragmatic Programmer, debate raged in computer magazines and online forums over the burning question: “Who would win the desktop GUI wars, Motif or OpenLook?”[36] It was the wrong question. Odds are you’ve probably never heard of these technologies as neither “won” and the browser-centric web quickly dominated the landscape.

Tip 43 Avoid Fortune-Telling

Much of the time, tomorrow looks a lot like today. But don’t count on it.


Related Sections Include


Topic 12, ​Tracer Bullets​

Topic 13, ​Prototypes and Post-it Notes​

Topic 40, ​Refactoring​

Topic 41, ​Test to Code​

Topic 48, ​The Essence of Agility​

Topic 50, ​Coconuts Don’t Cut It​


Footnotes


[30]

Based in part on earlier work by Dijkstra, Floyd, Hoare, Wirth, and others.


[31]

In C and C++ these are usually implemented as macros. In Java, assertions are disabled by default. Invoke the Java VM with the –enableassertions flag to enable them, and leave them enabled.


[32]

http://www.eps.mcgill.ca/jargon/jargon.html#heisenbug


[33]

For a discussion of the dangers of coupled code, see Topic 28, ​Decoupling​.


[34]

See the tip here.


[35]

Per the NHTSA, Stopping Distance = Reaction Distance + Braking Distance, assuming an average reaction time of 1.5s and deceleration of 17.02ft/s².


[36]

Motif and OpenLook were GUI standards for X-Windows based Unix workstations.